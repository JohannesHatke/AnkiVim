Representation of Gaussians (moments vs canonical)  	[latex]Moments $(\mu, \Sigma)$:\\\\$\Sigma = \Omega^{-1}\\$\\$\mu = \Omega^{-1}\ \xi$\\ \\ \\Canonical (information matrix $\Omega$ and information vector $\xi$):\\\\$\Omega = \Sigma^{-1}$\\$\xi = \Sigma^{-1}\ \mu$[/latex]
Deriving the information form (EIF)  	[latex]\begin{align*}p(x) &= det(2 \pi \Sigma)^{-\frac{1}{2}}exp(-\frac{1}{2}(x-\mu)^\top\Sigma^{-1} (x-\mu))\\&= det(2 \pi \Sigma)^{-\frac{1}{2}} exp(-\frac{1}{2} x^\top \Sigma^{-1}x +x^\top \Sigma^{-1} \mu - \frac{1}{2} \mu^\top \Sigma^{-1} \mu \\&= det(2 \pi \Sigma)^{-\frac{1}{2}} exp(-\frac{1}{2} \mu^\top \Sigma^{-1} \mu)exp(-\frac{1}{2} x^\top \Sigma^{-1} x + x^\top \Sigma^{-1} \mu)\\&= \eta exp(-\frac{1}{2} x^\top \Sigma^{-1} x + x^\top \Sigma^{-1} \mu)\\&= \eta exp(-\frac{1}{2} x^\top \Omega x + x^\top \xi)\end{align*}[/latex]
Marginalization and Conditioning  	TODO: INSERT IMAGE SLIDE 12
Properties Comparison: Kalman Filter vs Information Filter  	-> Two parameterization for Gaussian-> Same expressiveness-> Marginalization + conditioning different complexities
Prediction step (EIF Slam) 	[latex]\begin{equation}\bar{\Omega}_t = (A_t \Omega_{t-1}^{1} A_{t}^\top + R_t)^{-1}\end{equation}Justification for this; in Kalman Filter, we have:\\$\bar{\Sigma_t} = A_t \Sigma_{t-1} A_{t}^\top + R_t$\\and we use that: $\Sigma_{t-1} = \Omega_{t-1}^{-1}$ \\ \\\begin{equation}\bar{\xi_t} = \bar{\Omega_{t}} (A_t \Omega_{t-1}^{-1} \xi_{t-1} + B_t u_t)\end{equation}Justification for this; in Kalman Filter, we have:\\$\bar{mu_t} = A_t \mu_{t-1} + B_t u_t$\\and we use that: $\bar{\mu_{t-1}} = \Omega_{t-1}^{-1} \xi_{t-1}$[/latex]
EIF SLAM Correction step: Derivation  	[latex]Idea: Use Bayes Filter measurements update and replace components\\ \\\begin{align*}bel(x_t) &= \eta\ p(z_t|x_t)\ \bar{bel}(x_t)\\&= \eta' (-\frac{1}{2} (z_t - C_t x_t)^\top Q_{t}^{-1} (z_t - C_t x_t)) exp(-\frac{1}{2} (x_t - \bar{\mu}_t)^\top \bar{\Sigma_t}^{-1} (x_t -\bar{\mu_t}))\\&= \eta' exp (-\frac{1}{2} (z_t - C_t x_t)^\top Q_{t}^{-1} (z_t - C_t x_t) -\frac{1}{2} (x_t - \bar{\mu_t})^\top \bar{\Sigma_t}^{-1} (x_t - \bar{\mu_t}))\\&= \eta'' exp(-\frac{1}{2} x_{t}^\top C_{t}^\top Q_{t}^{-1} C_t x_t +x_{t}^\top C_{t}^\top Q_{t}^{-1} z_t - \frac{1}{2} x_{t}^\top \bar{\Omega_t}x_t+ x_{t}^\top \bar{\xi_t})\\&= \eta'' exp (-\frac{1}{2} x_{t}^\top [C_{t}^\top Q_{t}^{-1} C_t +\bar{\Omega_t}] x_t + x_{t}^\top [C_{t}^\top Q_{t}^{-1} z_t + \bar{\xi_t}]\\&= \eta'' exp(-\frac{1}{2}x_{t}^\top \Omega_t x_t + x_{t}^\top \xi_t\end{align*}[/latex]
Correction step: Update Rule (no derivation)  	[latex]\begin{align*}\Omega_t &= C_{t}^\top Q_{t}^{-1}C_t + \bar{\Omega_t}\\\xi_t    &= C_{t}^\top Q_{t}^{-1} z_t + \xi_t\end{align*}[/latex]
Complexity properties: Kalman Filter vs Information Filter  	[latex]Kalman Filter:\begin{itemize}\item prediction step efficient: $\mathcal{O}(n^2)$\item correction step costly: $\mathcal{O}(n^2 + k^{2.4}$\end{itemize}Information Filter:\begin{itemize}\item prediction step costly: $\mathcal{O}(n^{2.4})$\item correction step efficient: $\mathcal{O}(n^2)$ (can even be faster, especially in SLAM; depending on type of controls+observations)\end{itemize}Transformations between Kalman Filter parameterization and Info Filterparameterization costly:\\\\\begin{equation*}\mathcal{O}(n^{2.4})\end{equation*}[/latex]
