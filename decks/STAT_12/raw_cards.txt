Motivation: Sampling Methods  	[latex]restricting to distributions that can be handled analytically (orapproximated analytically), has drawbacks:\begin{enumerate}    \item mainly restricted to Gaussian distributions    \item priors must be conjugate    \item marginal distributions must be avoided (e.g. evidence approximation)\end{enumerate}Reasons for this:\begin{enumerate}    \item nonlinear or non-convex models $=$ cannot solve equations to obtain          optimal parameters    \item evaluation of integrals of marginal distribution is intractable if          there is no parametric form of the integral expression.\end{enumerate}Problem this induces $=$ restricted model may not fit problem well\\ \\$\Rightarrow$ solution $=$ local optimization (variational methods) or sampling[/latex]
Sampling (Monte-Carlo) Methods: General Idea  	[latex]\begin{enumerate}    \item basic idea: often we don't care about actual posterior but in its          maximum or expectation    \item expectation yields optimum decision according to common loss functions\end{enumerate}So, we would like to estimate expected loss:\begin{equation*}\mathbb{E}[f] = \int f(z) p(z)\end{equation*}where $p(z)$ is joint distribution of general (usually complex) model.\\ \\Concept:\begin{itemize}    \item Draw numbers of samples $z^{(l)}$ independently from $p(z)$ and usethem to approximate expectation by finite sum:\begin{equation*}\bar{f} = \frac{1}{L} \sum \limits_{l=1}^{L} f(z^{(l)})\end{equation*}\end{itemize}[/latex]
Number of Samples: Problems with Sampling based Methods  	[latex]\begin{itemize}    \item few samples can be sufficient to estimate expectation, independent of          dimensionality of sampling vector $z$\end{itemize}\begin{enumerate}    \item Problem: $z^{(l)}$ should be independent samples from $p(z)$    \begin{itemize}        \item difficult to create such samples from arbitrary distribution        \item If samples not drawn independently, effective number of samples is              much smaller $\Rightarrow$ might need thousand of (bad) samples    \end{itemize}    \item Problem: expectation may be dominated by regions of small probability          $\Rightarrow$ more samples necessary\end{enumerate}TODO: INSERT IMAGE FROM SLIDE 4[/latex]
Sampling from standard distributions  	[latex]\begin{itemize}    \item suppose we have random number generator that gives uniformly          distributed random numbers  $z$ from $[0,1]$    \item major task in sampling methods: transform samples $z$ from initial          distribution $p(z)$ into samples $y$ of target distribution $p(y)$    \item if functional description of $p(y)$ exists, we can try to findfunction $y = h(z)$ that transforms samples from $p(z)$ to samples from $p(y)$.\end{itemize}Derivation:\begin{align*}p(y) \frac{dy}{dz} = p(z) &\iff p(y) dy = p(z) dz\\\int \limits_{-\infty}{y} p(y')dy' = &\int \limits_{-\infty}{z} p(z')dz' = z =h^{-1}(y)\end{align*}$\Rightarrow$ we must compute inverse function of integral over $p(y)$\\$\Rightarrow$ for this to work, we must be able to write down distribution, itsintegral and the inverse function\\$\Rightarrow$ not possible in general[/latex]
Rejection Sampling  	[latex]If there is standard distribution $q(z)$ (from which we can sample directly)and fixed factor $k$ s.t.:\begin{equation*}p(z) \leq kq(z)\ \forall z\end{equation*}we can apply rejection sampling.\begin{enumerate}    \item draw sample $z_0$ from $q(z)$    \item draw sample $u_0$ from uniform distribution in interval $[0, kq(z_0)]$    \item reject sample if $u_0 > p(z_0)$ else accept it\end{enumerate}[/latex]
 Rejection Sampling: Acceptance rate 	[latex]\begin{itemize}    \item acceptance rate depends on good choice of $k$:\begin{equation*}p(accept) = \int \frac{p(z)}{kq(z)}q(z)dz = \frac{1}{k} \int p(z)dz =\frac{1}{k}\end{equation*}\item if $p(z)$ highly peaked, $k$ must be large and acceptance rate gets poor\item acceptance rate decreasesexponentially with dimensionality of samples $\Rightarrow$ sampling in high dimensional spaces hard $=$ only inlow-dimensional spaces\end{itemize}[/latex]
Importance Sampling  	[latex]\begin{itemize}    \item contrast to rejection sampling: don't need exact bounds forapproximating distribution $q(z)$    \item if approximating distribution is bad, weights of samples will be small    \item only few weights may be significantly large, leading to smalleffective sample size:\begin{equation*}L_{eff} = \sum \limits_{l=1}^{L} \frac{p(z^{(l)}}{q(z^{(l)}}\end{equation*}    \item if only small weights: variance small but approximated expectationmight be totally wrong $\Rightarrow$ difficult to judge quality of result\end{itemize}[/latex]
Markov chain Monte Carlo (MCMC) sampling  	[latex]\begin{itemize}    \item other sampling method: independent samples but problems findingenough significant samples in high-dimensional spaces    \item MCMC find many significant samples also in high-dimensional spaces tcost of samples being mutually dependent    \item basic technique: Metropolis algorithm\end{itemize}Metropolis algorithm:\begin{enumerate}    \item draw $z^{(0)}$ from  proposal distribution $q(z)$    \item repeat sampling steps:\begin{itemize}    \item draw sample from $q(z|z^{(l)})$\item accept if:\begin{equation*}min(1, \frac{p(z^{(l+1)}}{p(z^{(l)}})) > u\end{equation*}where $u$ is sample from $[0,1]$ uniform distribution\item else discard $z^{(l+1)}$\end{itemize}\end{enumerate}samples form markov chain[/latex]
 MCMC sampling: properties 	[latex]\begin{itemize}    \item can be used to sample from anisotropic gaussian via isotropic           gaussian proposal    \item corresponds to stochastic hill climbing or gradient descent    \item due to random component, method can theoretically pass valleys indistribution $+$ overcome local optima    \item if areas between valleys large, this is unlikely\end{itemize}[/latex]
Gibbs sampling  	[latex]\begin{enumerate}    \item popular special case of Metropolis algorithm    \item goal: sample from joint distribution $p(z_1, \dots, z_M)$ by cyclingthrough variables:\begin{align*}z_{1}^{(l+1)} &\sim p(z_{1} |z_{2}^{(l)}, \dots z_{M}^{(l)}\\z_{2}^{(l+1)} &\sim p(z_{2} |z_{1}^{(l+1)}, z_{3}^{(l)} \dots z_{M}^{(l)}\\&...\\z_{M}^{(l+1)} &\sim p(z_{M} |z_{1}^{(l+1)}, \dots z_{M-1}^{(l+1)}\\\end{align*}\item related to coordinate ascent (stochastic version of local optimizationtechnique)\end{enumerate}[/latex]
