Graphical models  	[latex]Motivation:\\\begin{itemize}    \item if probabilistic models get complex, write it down not as formula butas graph    \item graph consists of nodes representing variables and edges representingdependencies between variables $=$ Bayesian networks\end{itemize}TODO: TOPIMAGE SLIDE 2[/latex]
Directed graphs  	[latex]\begin{enumerate}    \item joint distribution can be derived from graph by general formula:\begin{equation*}p(x) = \prod \limits_{k=1}^{K} p(x_k | \mathcal{P}(x_k))\end{equation*}where $K$ is number of nodes and $\mathcal{P}(x_k)$ denotes parent nodes ofnode $x_k$\item important requirement for this rule: graph has no directed cycles $=$directed acyclic graph (DAG)\end{enumerate}[/latex]
Graphical Models: How to represent linear regression?  	TODO: INSERT IMAGE FROM SLIDE 4 SLIDESET 11
Latent and observed variables (graphical models) 	[latex]\begin{itemize}    \item observed variables $=$ filled circle    \item latent variables $=$ unfilled circle    \item predictions can be included, since they are not observed: predicationvariables considered latent variables (unfilled circles)\end{itemize}[/latex]TODO: INSERT IMAGE FROM SLIDE 5 SLIDESET 11
Independence and model complexity  	[latex]\begin{enumerate}    \item number of free model parameters relates to degree of interdependence          between variables    \item Example: if two gaussian distributed variables independent, then          covariance matrix is diagonal matrix    \item If all variables depend on each other, covariance matrix is fullyoccupied    \item intermediate cases $=$ some off-diagonal elements zero    \item independence assumptions reduce space of possible joint distributions\end{enumerate}[/latex]
Conditional Independence  	[latex]\begin{enumerate}    \item Independence of variables can hold under conditions $=$ conditional          independence     \item If $p(a|b,c) = p(a|c)$ then $a$ is conditionally independent of $b$           given $c$     \item Looking at joint distribution of $a$ and $b$:\begin{equation*}p(a, b|c) = p(a|b,c) p(b|c) = p(a|c) p(b|c)\end{equation*}we further see that variables $a$ and $b$ are independent (they factorize)conditioned on $c$.\item Graphical models directly show conditional independence properties ingraph structure $=$ d-separation (three cases)\end{enumerate}[/latex]
Conditional Independence - Case I: tail-to-tail 	[latex]\begin{enumerate}    \item graph corresponds to joint distribution:\begin{equation*}p(a,b|c) = p(a|c) p(b|c)\end{equation*}\item If $c$ not observed, joint distribution of $a, b$ is:\begin{equation*}p(a,b) = \int p(a|c) p(b|c) p(c) dc\end{equation*}and generally does not factorize into the product $p(a) p(b)$\item If $c$ is observed, we obtain Conditional independence between $a$ and $b$\item It is said that the observed variable blocks path between $a$ and $b$\end{enumerate}[/latex]TODO:  INSERT IMAGE FROM SLIDEE 8 SLIDESET 11
Conditional Independence - Case II: head-to-tail 	[latex]\begin{enumerate}    \item here joint distribution reads:$p(a,b,c) = p(a)p(c|a)p(b|c)$    \item If $c$ not observed, we generally do not have factorization:\begin{equation*}p(a,b) = p(a) \int p(c|a) p(b|c) dc = p(a) p(b|a)\end{equation*}\item observation of $c$ blocks path between $a$ and $b$ and leads toconditional independence of $a$ and $b$:\end{enumerate}[/latex]TODO: INSERT LAST TERM DERIVATION FROM SLIDE 9 SLIDESET 11 AND IMAGES
Conditional Independence - Case III: head-to-head 	[latex]\begin{enumerate}    \item join probability in this case:\begin{equation*}p(a,b,c) = p(a) p(b) p(c|a,b)\end{equation*}\item If $c$ not observed, $a$ and $b$ independent:\begin{equation*}p(a,b) = p(a) p(b)\end{equation*}\item vice-versa, if $c$ (or one of its descendants) observed, Conditionaldistribution is not independent:\begin{equation*}p(a,b|c) = \frac{p(a,b,c)}{p(c)} = \frac{p(a)p(b)p(c|a,b)}{p(c)}\end{equation*}\item opposite to case I and case II\item Example: $a =$ "you drank too much last night, $b =$ "you are sitting ina tough class", $c=$ "you have a headache". $a, b$ a priori independent.Knowing $c$ increases probability for the possible reasons, but one of thereasons is enough to explain $c$\\$\Rightarrow a,b$ influence each other in explaining observation $c$\end{enumerate}[/latex]
D-separation for a general graph  	[latex]\begin{itemize}    \item For a general graph, two nodes $a,b$ are independent if path betweenthem is either:\begin{enumerate}    \item blocked head-to-tail or tail-to-tail by node $c$ and this node isobserved (conditional independence)    \item blocked head-to-head by node $c$ and this node as well as itsdescendants are not observed.\end{enumerate}\end{itemize}[/latex]TODO:  INSERT IMAGE WITH CAPTION FROM SLIDE 11 SLIDESET 11
Undirected Graphs  	[latex]\begin{enumerate}    \item Alternatively to directed graphs, models can be represented byundirected graphs $=$ Markov random fields (Computer vision)    \item factorization properties of such graphs described by cliques    \item clique $=$ fully connected subset of nodes     \item maximal clique is clique st. no further node can be added withoutlosing clique property    \item undirected graph factorizes into its maximal cliques (does not meanthe single nodes factorize)\end{enumerate}[/latex]
Potential functions  	[latex]\begin{enumerate}    \item each clique $C$ described by strictly positive potential function$\psi_C(x_C)$    \item usually expressed as exponentials:\begin{equation*}\psi_C(x_C) = \exp(-E(x_C))\end{equation*}where $E(x_C)$ called energy function.    \item joint distribution can then be written as product over all cliques:    \begin{equation*}p(x) = \frac{1}{Z} \prod \limits_C \psi_C(x_C)    \end{equation*}    \item Note that potential functions are in general not normalized (otherthan probabilities in directed graphs). Therefore we must normalize jointdistribution manually by the partition function:\begin{equation*}Z = \sum \limits_x \prod \limitts_C \psi_C(x_C)\end{equation*}            \end{enumerate}[/latex]
Inference in graphical models  	[latex]\begin{enumerate}    \item most important advantage of graphical representations is possibilityto derive efficient algorithms for inference that make use of factorizationproperties    \item if graph is tree, there is very efficient, exact inference algorithmcalled sum-product algorithm/belief propagation.\end{enumerate}[/latex]TODO: INSERT IMAGE WITH TEXT ABOVE AND TEXT AND TERM BELOW FROM SLIDE 14SLIDESET 11
Inference on a chain  	[latex]\begin{enumerate}    \item in naive implementation we would evaluate joint distribution and thenperform summations:\begin{equation*}p(x_n) = \sum \limits_{x_1} \dots \sum \limits_{x_{n-1}} \sum \limits_{x_{n+1}}\dots \sum \limits_{x_N} p(x)\end{equation*}\item joint distribution has $K^N$ states and so evaluation scalesexponentially with number of variables\item like ignoring all conditional independence in the graph\item If we make use of factorization, we see that for summation over $x_N$only one potential plays a role: $\psi_{N-1, N}(x_{N-1}, x_N)$\item We must only evaluate $\sum \limits_{x_N} \psi_{N-1, N}(x_{N-1}, x_N)$ to eliminate $x_N$.\end{enumerate}[/latex]
Message passing  	[latex]\begin{enumerate}    \item starting at the front and the end of the chain we can recursivelyeliminate variables until we arre only left with two expressions$\mu_{\alpha}(x_n)$ and $\mu_{\beta}(x_n)$ that depend on $x_n$\item functions obtained in each elimination step are called messages andrecursive scheme is called messsage passing\item each potential function evaluation requires looking at $K\times K$ table.So each elimination has cost $O(K^2)$. Eliminating all but one variablestherefore has cost $O(NK^2)$\item marginal is finally obtained as:\begin{equation*}p(x_n) = \frac{1}{Z} \mu_{\alpha}(x_n) \mu_{\beta}(x_n) =\frac{\mu_{\alpha}(x_n) \mu_{\beta}(x_n)}{\sum \limits_{x_n} \mu_{\alpha}(x_n)\mu_{\beta}(x_n)\end{equation*}\end{enumerate}[/latex]
Efficiently computing marginals of all variables -- Graphical Inference  	[latex]\begin{enumerate}    \item If we wish to compute the marginal of a second variable, we could usemessage passing from scratch. Such a procedure would cost $O(N^2K^2)$ tocompute the marginals of all variables ($N$ times $O(NK^2)$ for single messagepassing)\item In fact, many computations would happen doubled.\item It is more efficient to first launch a message $\mu_{\beta}(x_{N-1})$starting from node $x_N$ and propagate it all the way back to node $x_1$.Similarly we launch a message $\mu_{\alpha}(x_2)$ starting from node $x_1$ andpropagate it to node $x_N$.\item Saving all the messages on the way, we can compute all marginals via:\begin{equation*}p(x_i) = \frac{1}{Z} \mu_{\alpha}(x_i) \mu_{\beta}(x_i)\end{equation*}\item This takes only twice time as computing marginal of single node.\item same concept can also be used for trees.\end{enumerate}[/latex]
Tree-structured graphs  	[latex]\begin{enumerate}    \item A graph is a tree if there is exactly one path between any pair ofnodes (no loops)    \item For dealing with directed/undirected trees in unique way, we usefactor graphs $\Rightarrow$ introduces additional nodes for the factorsappearing from the factorization properties of the graph\end{enumerate}[/latex]
