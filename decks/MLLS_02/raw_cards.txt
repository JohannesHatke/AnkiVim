 &nbsp;<div><br /></div>Supervised Problem -- Descriptive models&nbsp;<div><br /></div> &nbsp;<div><br /></div>	 &nbsp;<div><br /></div>[latex]&nbsp;<div><br /></div>\begin{enumerate}&nbsp;<div><br /></div>\item instances $=$ sets of variables (inputs / features); gathered from&nbsp;<div><br /></div>measurements&nbsp;<div><br /></div>\item (one or more) unknown output variables&nbsp;<div><br /></div>\item belief: input variables influence output variable(s)&nbsp;<div><br /></div>\end{enumerate}&nbsp;<div><br /></div>$\Rightarrow$ Supervised learning is task of using input to predict output&nbsp;<div><br /></div>&nbsp;<div><br /></div>[/latex] &nbsp;<div><br /></div>
Variabe Types + Terminology -- Descriptive Models &nbsp;<div><br /></div> &nbsp;<div><br /></div>	 &nbsp;<div><br /></div>[latex]&nbsp;<div><br /></div>\begin{enumerate}&nbsp;<div><br /></div>\item Variables $=$ quantitative (numbers) / qualitative /categorical  measurements&nbsp;<div><br /></div>\item regression task $=$ output quantitative&nbsp;<div><br /></div>\item classification task $=$ output qualitative&nbsp;<div><br /></div>\item ordered categorical $=$ e.g. T-shirt values (S, M, L)&nbsp;<div><br /></div>\item qualitative variables often represented by numerical codes $=$ targets,&nbsp;<div><br /></div>e.g. binary variable by $-1$ or $1$&nbsp;<div><br /></div>\item more than two categories: one-hot-encoding: $k$-levels variable&nbsp;<div><br /></div>represented by binary vector of $k$ bits only one of which is true at the same&nbsp;<div><br /></div>time&nbsp;<div><br /></div>\end{enumerate}&nbsp;<div><br /></div>[/latex] &nbsp;<div><br /></div>
Notations (input/output variables...) -- Descriptive models &nbsp;<div><br /></div> &nbsp;<div><br /></div>	 &nbsp;<div><br /></div>[latex]&nbsp;<div><br /></div>\begin{enumerate}&nbsp;<div><br /></div>\item $X$: input variable, $X_j$: j-th component of vector $X$&nbsp;<div><br /></div>\item $Y$: quantitative output, $G$: qualitative output&nbsp;<div><br /></div>\item $x_i$: i-th observed value of $X$ (scalar or vector)&nbsp;<div><br /></div>\item $\mathbb{X}: N \times p$ matrix of $N$ vectors $x_i$ each with $p$&nbsp;<div><br /></div>components&nbsp;<div><br /></div>\item all vectors column vectors $\rightarrow$ $i$-th row of $\mathbb{X}$ is &nbsp;<div><br /></div>$x_{i}^{\top}$&nbsp;<div><br /></div>\item learning task: make good prediction $\hat{Y}$ of $Y$ given inputs $X$&nbsp;<div><br /></div>\item to construct predictor, need many measurements $(x_i, y_i)$: called&nbsp;<div><br /></div>training data&nbsp;<div><br /></div>\end{enumerate}&nbsp;<div><br /></div>[/latex] &nbsp;<div><br /></div>
Linear models: model definition -- Descriptive models &nbsp;<div><br /></div> &nbsp;<div><br /></div>	 &nbsp;<div><br /></div>[latex]&nbsp;<div><br /></div>\begin{itemize}&nbsp;<div><br /></div>\item Given: vector of inputs: $X^\top = (X_1, \dots, X_p)$&nbsp;<div><br /></div>\item model definition:&nbsp;<div><br /></div>\begin{equation*}&nbsp;<div><br /></div>\hat{Y} = \hat{\beta}_0 + \sum \limits_{j=1}^{p}X_j \hat{\beta}_j&nbsp;<div><br /></div>\end{equation*}&nbsp;<div><br /></div>\item extending $X$ with constant variable $1$ allows rewriting the model as&nbsp;<div><br /></div>the inner product equivalently:&nbsp;<div><br /></div>\begin{equation*}&nbsp;<div><br /></div>\hat{Y} = X^\top \hat{\beta}&nbsp;<div><br /></div>\end{equation*}&nbsp;<div><br /></div>\item if $\hat{Y}$ is vector, than $\hat{\beta}$ is matrix&nbsp;<div><br /></div>\item $(X, \hat{Y})$ is hyper-plane in $(p+1)$-dimensional space&nbsp;<div><br /></div>\end{itemize}&nbsp;<div><br /></div>[/latex] &nbsp;<div><br /></div>
How can we find the linear model that best fits the data? &nbsp;<div><br /></div>-- Descriptive models&nbsp;<div><br /></div> &nbsp;<div><br /></div>	 &nbsp;<div><br /></div>[latex]&nbsp;<div><br /></div>method of least squares&nbsp;<div><br /></div>\begin{itemize}&nbsp;<div><br /></div>\item residual sum of squares:&nbsp;<div><br /></div>\begin{equation*}&nbsp;<div><br /></div>RSS(\beta) = \sum \limits_{i=1}^{N}(y_i - x_{i}^{\top} \beta)^2&nbsp;<div><br /></div>\end{equation*}&nbsp;<div><br /></div>or in matrix notation:&nbsp;<div><br /></div>\begin{equation*}&nbsp;<div><br /></div>RSS(\beta) = (y-\mathbb{X}\beta)^\top (y-\mathbb{X}\beta)&nbsp;<div><br /></div>\end{equation*}&nbsp;<div><br /></div>\end{itemize}&nbsp;<div><br /></div>where $\mathbb{X}$ is a $N \times p$ matrix with each row an input vector and&nbsp;<div><br /></div>$y$ is a $N$-vector of outputs in training set.&nbsp;<div><br /></div>[/latex] &nbsp;<div><br /></div>
[latex]&nbsp;<div><br /></div>Minimizing $RSS(\beta)$ for linear models -- Descriptive models&nbsp;<div><br /></div>[/latex]&nbsp;<div><br /></div> &nbsp;<div><br /></div>	 &nbsp;<div><br /></div>[latex]&nbsp;<div><br /></div>\begin{enumerate}&nbsp;<div><br /></div>\item minimum $RSS(\beta)$ is achieved when gradient wrt. $\beta$ vanishes:&nbsp;<div><br /></div>\begin{equation*}&nbsp;<div><br /></div>\mathbb{X}^\top (y-\mathbb{X}\beta) = 0&nbsp;<div><br /></div>\end{equation*}&nbsp;<div><br /></div>which is for:&nbsp;<div><br /></div>\begin{equation*}&nbsp;<div><br /></div>\hat{\beta} = (\mathbb{X}^{\top} \mathbb{X})^{-1} \mathbb{X}^\top y&nbsp;<div><br /></div>\end{equation*}&nbsp;<div><br /></div>\item for an arbitrary instance $x_0$, the predicted value is:&nbsp;<div><br /></div>\begin{equation*}&nbsp;<div><br /></div>\hat{y}_0 = x_{0}^\top \hat{\beta}&nbsp;<div><br /></div>\end{equation*}&nbsp;<div><br /></div>\end{enumerate}&nbsp;<div><br /></div>[/latex] &nbsp;<div><br /></div>
Nearest neighbor models: model formulation -- Descriptive models &nbsp;<div><br /></div> &nbsp;<div><br /></div>	 &nbsp;<div><br /></div>[latex]&nbsp;<div><br /></div>\begin{itemize}&nbsp;<div><br /></div>\item given instance $X$ the $k$-NN model uses $k$ closest instances in&nbsp;<div><br /></div>training set to compute $\hat{Y}$ as average of corresponding targets&nbsp;<div><br /></div>\item model is:&nbsp;<div><br /></div>\begin{equation*}&nbsp;<div><br /></div>\hat{Y}(x) = \frac{1}{k} \sum \limits_{x_i \in N_k(x)}y_i&nbsp;<div><br /></div>\end{equation*}&nbsp;<div><br /></div>\end{itemize}&nbsp;<div><br /></div>where $N_k(x)$ is set of $k$ closest points $x_i$ in training sample (closeness&nbsp;<div><br /></div>defined via metric e.g. Euclidean distance)&nbsp;<div><br /></div>[/latex] &nbsp;<div><br /></div>
No. of effective parameters of k-NN -- Descriptive models &nbsp;<div><br /></div> &nbsp;<div><br /></div>	 &nbsp;<div><br /></div>[latex]&nbsp;<div><br /></div>\begin{itemize}&nbsp;<div><br /></div>\item k-NN seems to have only one parameter: $k$&nbsp;<div><br /></div>\item however: consider there can be only $\frac{N}{k}$ non-overlapping&nbsp;<div><br /></div>neighborhoods and each one can be specified by one parameter (the center)&nbsp;<div><br /></div>\item $\rightarrow$ no. of effective parameters is $\frac{N}{k}$&nbsp;<div><br /></div>\end{itemize}&nbsp;<div><br /></div>[/latex] &nbsp;<div><br /></div>
Summary: Linear models vs k-NN  -- Descriptive models&nbsp;<div><br /></div> &nbsp;<div><br /></div>	 &nbsp;<div><br /></div>[latex]&nbsp;<div><br /></div>\begin{enumerate}&nbsp;<div><br /></div>\item linear models $=$ smooth and stable decision boundary, but require&nbsp;<div><br /></div>dataset to be linearly seperable for good results&nbsp;<div><br /></div>\item k-NN $=$ arbitrary complex decision boundary but changing few points can&nbsp;<div><br /></div>completely change prediction (unstable)&nbsp;<div><br /></div>\item $\Rightarrow$ linear model $=$ high bias, low variance&nbsp;<div><br /></div>\item $\Rightarrow$ k-NN $=$ low bias, high variance&nbsp;<div><br /></div>\end{enumerate}&nbsp;<div><br /></div>[/latex] &nbsp;<div><br /></div>
