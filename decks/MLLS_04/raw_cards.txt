Bias/Variance: Intuition -- Bias+Variance &nbsp;<div><br /></div> &nbsp;<div><br /></div>	 &nbsp;<div><br /></div>[latex]&nbsp;<div><br /></div>\begin{enumerate}&nbsp;<div><br /></div>\item bias $=$ error due to difference between \emph{true} function and&nbsp;<div><br /></div>\emph{model} function that we can represent or compute&nbsp;<div><br /></div>\item variance $=$ estimation error due to having finite sample to train our&nbsp;<div><br /></div>models with&nbsp;<div><br /></div>\end{enumerate}&nbsp;<div><br /></div>[/latex] &nbsp;<div><br /></div>
Expected value -- Bias+Variance &nbsp;<div><br /></div> &nbsp;<div><br /></div>	 &nbsp;<div><br /></div>[latex]&nbsp;<div><br /></div>\begin{equation*}&nbsp;<div><br /></div>E[x] = \sum \limits_{x \in \mathcal{X}} x p(x)&nbsp;<div><br /></div>\end{equation*}&nbsp;<div><br /></div>denoted with symbol $\mu$&nbsp;<div><br /></div>[/latex] &nbsp;<div><br /></div>
Expected value is linear operator -- Bias+Variance &nbsp;<div><br /></div>&nbsp;<div><br /></div> &nbsp;<div><br /></div>	 &nbsp;<div><br /></div>[latex]&nbsp;<div><br /></div>\begin{itemize}&nbsp;<div><br /></div>\item computing expected value is linear:&nbsp;<div><br /></div>\begin{equation*}&nbsp;<div><br /></div>E[\alpha_1 f_1(x) + \alpha_2 f_2(x)] = \alpha_1 E[f_1(x)] + \alpha_2 E[f_2(x)]&nbsp;<div><br /></div>\end{equation*}&nbsp;<div><br /></div>$\Rightarrow E$ is linear operator&nbsp;<div><br /></div>\end{itemize}&nbsp;<div><br /></div>[/latex] &nbsp;<div><br /></div>
Special cases expected value -- Bias + Variance &nbsp;<div><br /></div> &nbsp;<div><br /></div>	 &nbsp;<div><br /></div>[latex]&nbsp;<div><br /></div>\begin{enumerate}&nbsp;<div><br /></div>\item second moment:&nbsp;<div><br /></div>\begin{equation*}&nbsp;<div><br /></div>E[x^2] = \sum \limits_{x \in \mathcal{X}} x^2p(x)&nbsp;<div><br /></div>\end{equation*}&nbsp;<div><br /></div>\item variance:&nbsp;<div><br /></div>\begin{equation*}&nbsp;<div><br /></div>E[(x-\mu)^2] = \sum \limits_{x \in \mathcal{X}} (x-\mu)^2 p(x)&nbsp;<div><br /></div>\end{equation*}&nbsp;<div><br /></div>\item variance indicated with $\sigma^2$ and standard deviation is $\sigma$&nbsp;<div><br /></div>\end{enumerate}&nbsp;<div><br /></div>[/latex] &nbsp;<div><br /></div>
Notes on standard deviation -- Bias+Variance &nbsp;<div><br /></div> &nbsp;<div><br /></div>	 &nbsp;<div><br /></div>[latex]&nbsp;<div><br /></div>\begin{enumerate}&nbsp;<div><br /></div>\item standard deviation measures how much values of $x$ tend to differ from&nbsp;<div><br /></div>their average $\mu$&nbsp;<div><br /></div>\item In general case, (loose) bound is given by Chebyshev's inequality:&nbsp;<div><br /></div>\begin{equation*}&nbsp;<div><br /></div>p(|x-\mu| > n \sigma) \leq \frac{1}{n^2}&nbsp;<div><br /></div>\end{equation*}&nbsp;<div><br /></div>\end{enumerate}&nbsp;<div><br /></div>[/latex] &nbsp;<div><br /></div>
Useful Equalities (Expected value) -- Bias + Variance &nbsp;<div><br /></div> &nbsp;<div><br /></div>	 &nbsp;<div><br /></div>[latex]&nbsp;<div><br /></div>\begin{enumerate}&nbsp;<div><br /></div>\item $E[(x-E[x])^2] = E[x^2] - (E[x])^2$&nbsp;<div><br /></div>\item $\sigma(x,y) = E[(x-E[x])(y-E[y])] = E[xy] - E[x] E[y]$&nbsp;<div><br /></div>hereby $\sigma(x, y)$ is called covariance&nbsp;<div><br /></div>\end{enumerate}&nbsp;<div><br /></div>[/latex] &nbsp;<div><br /></div>
Bias-Variance Theory: Goal --  Bias + Variance&nbsp;<div><br /></div> &nbsp;<div><br /></div>	 &nbsp;<div><br /></div>[latex]&nbsp;<div><br /></div>\begin{itemize}&nbsp;<div><br /></div>\item Aim: Decompose error of predictive system induced from finite data sample &nbsp;<div><br /></div>into meaningful components for insight&nbsp;<div><br /></div>\item Result: $\text{error} = \text{ bias}^2 + \text{ variance } + \text{&nbsp;<div><br /></div>irreducible (Bayes) error}$&nbsp;<div><br /></div>\end{itemize}&nbsp;<div><br /></div>[/latex] &nbsp;<div><br /></div>
Bias-Variance Theory: Regression -- Bias+Variance &nbsp;<div><br /></div> &nbsp;<div><br /></div>	 &nbsp;<div><br /></div>[latex]&nbsp;<div><br /></div>\begin{itemize}&nbsp;<div><br /></div>\item true function:&nbsp;<div><br /></div>\begin{equation*}&nbsp;<div><br /></div>y = f(x) + \epsilon&nbsp;<div><br /></div>\end{equation*}&nbsp;<div><br /></div>where $\epsilon = \mathcal{N}(0, \sigma^2)$ is Gaussian random variable&nbsp;<div><br /></div>\item fit model function $h(\dot)$ (hypothesis) to set of training examples $S$&nbsp;<div><br /></div>\item Idea: Given observation for data point $(x_0, y_0)$ with $y_0 = f(x_0) +&nbsp;<div><br /></div>\epsilon$ understand and decompose expected prediction error:&nbsp;<div><br /></div>\begin{equation*}&nbsp;<div><br /></div>E[(y_0 - h(x_0))^2]&nbsp;<div><br /></div>\end{equation*}&nbsp;<div><br /></div>(see decomposition anki card)&nbsp;<div><br /></div>\end{itemize}&nbsp;<div><br /></div>[/latex] &nbsp;<div><br /></div>
Bias-Variance Theory: Decomposition of expected average prediction error&nbsp;<div><br /></div>-- Bias+Variance&nbsp;<div><br /></div> &nbsp;<div><br /></div>	 &nbsp;<div><br /></div>[latex]&nbsp;<div><br /></div>expected average prediction error:&nbsp;<div><br /></div>\begin{equation*}&nbsp;<div><br /></div>E[(y_0 - h(x_0))^2]&nbsp;<div><br /></div>\end{equation*}&nbsp;<div><br /></div>can be decomposed as:&nbsp;<div><br /></div>\begin{align*}&nbsp;<div><br /></div>&\ E[(y_0 - h(x_0))^2]\\ &nbsp;<div><br /></div>&= E[\epsilon^2] + (f(x_0)-\bar{h}(x_0))^2 + E[(h(x_0) - h(x_0))^2]\\ &nbsp;<div><br /></div>&= \sigma^2 + Bias(h(x_0))^2 + Var(h(x_0))&nbsp;<div><br /></div>\end{align*}&nbsp;<div><br /></div>$\Rightarrow$ Expected prediction error $=$ Noise $+$ $\text{Bias}^2$ $+$ Variance&nbsp;<div><br /></div>[/latex] &nbsp;<div><br /></div>
Bias-Variance Theory: Variance, Bias, Noise (Terms + Intuition)&nbsp;<div><br /></div>-- Bias+Variance &nbsp;<div><br /></div> &nbsp;<div><br /></div>	 &nbsp;<div><br /></div>[latex]&nbsp;<div><br /></div>\begin{enumerate}&nbsp;<div><br /></div>\item Variance:&nbsp;<div><br /></div>\begin{equation*}&nbsp;<div><br /></div>E[(h(x_0) - \bar{h}(x_0))^2]&nbsp;<div><br /></div>\end{equation*}&nbsp;<div><br /></div>describes variability of prediction $h(x_0)$ when different training sets are&nbsp;<div><br /></div>used to fit model $h$&nbsp;<div><br /></div>\item Bias: &nbsp;<div><br /></div>\begin{equation*}&nbsp;<div><br /></div>(\bar{h}(x_0) -f(x_0))^2&nbsp;<div><br /></div>\end{equation*}&nbsp;<div><br /></div>describes difference between expected predicted value and true (but unknown)&nbsp;<div><br /></div>value&nbsp;<div><br /></div>\item Noise:&nbsp;<div><br /></div>\begin{equation*}&nbsp;<div><br /></div>E[(y_0 - f(x_0))^2] = \sigma^2&nbsp;<div><br /></div>\end{equation*}&nbsp;<div><br /></div>describes how much $y_0$ can differ from true $f(x_0)$ due to intrinsic&nbsp;<div><br /></div>uncertainties&nbsp;<div><br /></div>\end{enumerate}&nbsp;<div><br /></div>[/latex] &nbsp;<div><br /></div>
Measuring Bias and Variance: Bootstrap Sampling -- Bias+Variance &nbsp;<div><br /></div>&nbsp;<div><br /></div> &nbsp;<div><br /></div>	 &nbsp;<div><br /></div>[latex]&nbsp;<div><br /></div>\begin{itemize}&nbsp;<div><br /></div>\item Problem: To measure expected prediction error $=$ need to induce&nbsp;<div><br /></div>predictors from many training sets, but have only one&nbsp;<div><br /></div>\item Solution: simulate multiple training sets by bootstrap replicates&nbsp;<div><br /></div>\end{itemize}&nbsp;<div><br /></div>Bootstrap Sampling:&nbsp;<div><br /></div>\begin{enumerate}&nbsp;<div><br /></div>\item Given set of training examples: $S=\{(x_, y_i)\}$&nbsp;<div><br /></div>\item extract replicate:&nbsp;<div><br /></div>\begin{equation*}&nbsp;<div><br /></div>S'=\{x \mid x \text{ is drawn at random with replacements from } S\}&nbsp;<div><br /></div>\end{equation*}&nbsp;<div><br /></div>with $|S'| = |S|$&nbsp;<div><br /></div>\end{enumerate}&nbsp;<div><br /></div>[/latex] &nbsp;<div><br /></div>
 Measuring Bias and Variance: Algorithm -- Bias+Variance&nbsp;<div><br /></div> &nbsp;<div><br /></div>	[latex]&nbsp;<div><br /></div>\begin{enumerate}&nbsp;<div><br /></div>\item make $B$ bootstrap replicate of $S: S_1, \dots, S_B$&nbsp;<div><br /></div>\item use $S_b$ as training set and induce hypothesis $h_b$&nbsp;<div><br /></div>\item make out of bag set $T_b = S \backslash S_b$ i.e. all data instances that do not appear in $S_b$&nbsp;<div><br /></div>\item compute $h_b(x)$ for each $x$ in $T_b$ (indicate with $K$ their number)&nbsp;<div><br /></div>\item compute expected prediction $\bar{h}(x_0) = \frac{1}{K} \sum&nbsp;<div><br /></div>\limits_{b=1}^{K}h_b(x)$&nbsp;<div><br /></div>\item estimate $\text{bias}^2$ as $(\bar{h}(x) - y)^2$&nbsp;<div><br /></div>\item estimate $\text{variance}$ as $\frac{1}{K-1} \sum&nbsp;<div><br /></div>\limits_{b=1}^{K}(\bar{h}(x)-h_b(x))^2$&nbsp;<div><br /></div>\item assume noise is $0$&nbsp;<div><br /></div>\end{enumerate}&nbsp;<div><br /></div>Noise can be estimated if multiple pairs $(x_i, y_i)$ for same $x_i$ are given&nbsp;<div><br /></div>or by considering $y$ values of nearby $x$&nbsp;<div><br /></div>[/latex] &nbsp;<div><br /></div>
Bias-Variance Dilemma -- Bias+Variance &nbsp;<div><br /></div> &nbsp;<div><br /></div>	 &nbsp;<div><br /></div>[latex]&nbsp;<div><br /></div>\begin{itemize}&nbsp;<div><br /></div>\item more flexible hypothesis has low bias but higher variance&nbsp;<div><br /></div>\item for family of hypotheseses (e.g. $k$-NN for different $k$-values) we can&nbsp;<div><br /></div>increase flexibility of hypothesis (reduce $k$) we still observe increase of&nbsp;<div><br /></div>variance&nbsp;<div><br /></div>\end{itemize}&nbsp;<div><br /></div>[/latex] &nbsp;<div><br /></div>
General remarks (Bias-Variance Dilemma) -- Bias+Variance &nbsp;<div><br /></div> &nbsp;<div><br /></div>	 &nbsp;<div><br /></div>[latex]&nbsp;<div><br /></div>\begin{enumerate}&nbsp;<div><br /></div>\item to have low error generally bette to prefer low variance to low bias&nbsp;<div><br /></div>\item for given bias, variance can be diminished by increasing size of training&nbsp;<div><br /></div>set&nbsp;<div><br /></div>\item bias can be reduced by increasing complexity of model until perfect match&nbsp;<div><br /></div>with true underlying function is achieved&nbsp;<div><br /></div>\item error cannot be reduced below Bayes error&nbsp;<div><br /></div>$\Rightarrow$ only way to have zero bias and variance is to use correct true&nbsp;<div><br /></div>function (guess it without learning)&nbsp;<div><br /></div>\end{enumerate}&nbsp;<div><br /></div>[/latex] &nbsp;<div><br /></div>
Expected prediction error estimation -- Bias+Variance &nbsp;<div><br /></div> &nbsp;<div><br /></div>	 &nbsp;<div><br /></div>[latex]&nbsp;<div><br /></div>\begin{itemize}&nbsp;<div><br /></div>\item EPE can be estimated from error on independent test set&nbsp;<div><br /></div>\item error on training set instead is optimistic estimate of EPE (does not&nbsp;<div><br /></div>take into account model complexity)&nbsp;<div><br /></div>$\Rightarrow$ does not allow estimate variance component of error&nbsp;<div><br /></div>\item large variance implies large EPE&nbsp;<div><br /></div>\end{itemize}&nbsp;<div><br /></div>How can we obtain good estimate of EPE?&nbsp;<div><br /></div>\begin{itemize}&nbsp;<div><br /></div>\item use cross validation&nbsp;<div><br /></div>\end{itemize}&nbsp;<div><br /></div>[/latex] &nbsp;<div><br /></div>
Cross-Validation: Algorithm -- Bias+Variance &nbsp;<div><br /></div> &nbsp;<div><br /></div>	 &nbsp;<div><br /></div>[latex]&nbsp;<div><br /></div>\begin{enumerate}&nbsp;<div><br /></div>\item Randomly permute data&nbsp;<div><br /></div>\item split data into $K$ roughly equal sized parts with same distribution &nbsp;<div><br /></div>of targets as in the whole set&nbsp;<div><br /></div>\item Let $h^{-i}$ be hypothesis fitted to data with $i$-th part removed&nbsp;<div><br /></div>\item The cross-validation estimate of prediction error is:&nbsp;<div><br /></div>\begin{equation*}&nbsp;<div><br /></div>CV(h) = \frac{1}{N} \sum \limits_{i=1}^{N} L(y_i, h^{-i}(x_i))&nbsp;<div><br /></div>\end{equation*}&nbsp;<div><br /></div>\end{enumerate}&nbsp;<div><br /></div>&nbsp;<div><br /></div>&nbsp;<div><br /></div>[/latex] &nbsp;<div><br /></div>
Cross-Validation to select model with lowest expected prediction error &nbsp;<div><br /></div>-- Bias+Variance &nbsp;<div><br /></div>&nbsp;<div><br /></div> &nbsp;<div><br /></div>	 &nbsp;<div><br /></div>[latex]&nbsp;<div><br /></div>\begin{enumerate}&nbsp;<div><br /></div>\item Divide data set as before&nbsp;<div><br /></div>\item Let $h(x, \alpha)$ be a hypothesis for point $x$ under parameter $\alpha$&nbsp;<div><br /></div> (i.e. $\alpha$ can be number of neighbors in $k$-NN model&nbsp;<div><br /></div>\item Let $h^{-i}$ be hypothesis fitted to data set with $i$-th part removed&nbsp;<div><br /></div>\item cross-validation estimate of prediction error of $h$ with parameter &nbsp;<div><br /></div>$\alpha$ is:&nbsp;<div><br /></div>\begin{equation*}&nbsp;<div><br /></div>CV(h, \alpha) = \frac{1}{N} \sum \limits_{i=1}^{N} L(y_i, h^{-i}(x_i, \alpha))&nbsp;<div><br /></div>\end{equation*}&nbsp;<div><br /></div>\item choose model parameter $\alpha^*$ that minimizes $CV$&nbsp;<div><br /></div>\end{enumerate}&nbsp;<div><br /></div>[/latex] &nbsp;<div><br /></div>
Choice of cross-validation parameter K -- Bias+Variance &nbsp;<div><br /></div>&nbsp;<div><br /></div> &nbsp;<div><br /></div>	 &nbsp;<div><br /></div>[latex]&nbsp;<div><br /></div>Estimation of error by cross-validation is itself subject to bias-variance &nbsp;<div><br /></div>trade-off&nbsp;<div><br /></div>\begin{itemize}&nbsp;<div><br /></div>\item $K=N$ is unbiased estimtor of true error but has large variance&nbsp;<div><br /></div>\item $K=5$ variance low but bias can be high&nbsp;<div><br /></div>\item $K=10$ recommended as good compromise&nbsp;<div><br /></div>\end{itemize}&nbsp;<div><br /></div>[/latex] &nbsp;<div><br /></div>
Things that can be done wrong when doing cross-validation -- Bias+Variance&nbsp;<div><br /></div> &nbsp;<div><br /></div>	 &nbsp;<div><br /></div>[latex]&nbsp;<div><br /></div>\begin{enumerate}&nbsp;<div><br /></div>\item select features looking at all data&nbsp;<div><br /></div>\item information on class of test point available during test procedure&nbsp;<div><br /></div>\end{enumerate}&nbsp;<div><br /></div>[/latex] &nbsp;<div><br /></div>
