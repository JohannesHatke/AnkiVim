When run for 5 seconds, which algorithm is better: ILS or MMAS?Which algo's runtime distribution has lower 75 percent quantile?TODO: Insert image from slide 4 here 	TODO: INSERT qa answers from slide 4 video
Comparing algorithms: Which methods exist?  	- summary statistics = average runtime and no. of timeouts for example- runtime distribution plot across benchmark instances- scatter plot = each marker = one instance, log-log axis used=> can reveal patterns in data- boxplots
Scatter plots: There are 2 instance clustes: sat and unsat instances=> Which cluster is which? TODO: INSERT IMAGE WITH BULLET POINTS FROM SLIDE 12 	TODO: INSERT QA from SLIDE 12
Background: statistical hypothesis tests  	[latex]\begin{itemize}\item When having lots data = need summarizing\begin{itemize}\item but summarization hides data\item wanted: drawing high level conclusions (A outperforms B on instances oftype X)\end{itemize}\item Problem: only finite number of observations\begin{itemize}\item are observed performance differences due to chance?\item are we sure a claim we make can be reproduced?\end{itemize}$\Rightarrow$ statistical tests help\end{itemize}[/latex]
Example of statistical test: IQ values  	[latex]\begin{itemize}\item IQ values $\sim \mathcal{N}(100,15)$\item Null hypothesis: $H_0 = \mu = 100$\item \end{itemize}[/latex]
[latex]Null hypothesis: $H_0$[/latex] 	-> default position/prior knowledge about the distribution in question during astatistical test (often the mean of the distribution)-> hypothesis assumes => no relationship between studied phenomena, nodifference amongst groupse.g. IQ => mean over population = 100, for a particular group, the nullhypothesis would be that the mean IQ of the group is approximately 100
General principle: Statistical tests  	[latex]\begin{itemize}\item Compare test statistic $\bar{x}$ to its sampling distribution under $H_0$\item P-value: probability $P$ of observing values at least as extreme as$\bar{x}$\item Compare $p$ to pre-defined confidence level $\alpha$ (typically $\alpha =0.05$): if $p<a$, reject $H_0$\end{itemize}[/latex]
Statistical Tests: Z-test properties 	[latex]\begin{itemize}\item Assumption: $X \sim \mathcal{N}(\mu, \sigma^2)$, with known $\mu$ and$\sigma^2$\item $H_0: \mu = \mu_0, H_1: \mu > \mu_0$\item Test statistic: sample mean $\bar{x}$; evaluate under $\mathcal{N}(\mu =\mu_0, s=\frac{\sigma^2}{\sqrt{n}}$\item Equivalent: compute Z-statistic = $Z = \frac{\bar{x}-\mu_0)}{s}$ andevaluate cumulative density $\Phi(Z)$ of $Z$ under $\mathcal{N}(0,1)$(can be lookup in tables or using software)\end{itemize}[/latex]
Two-sided tests  	[latex]\begin{itemize}\item like one-sided tests, but test for extreme values in both tails\item ex.: Z-test: two sided alternative hypothesis $H_1 :  \mu > \mu_0 $ or$\mu < \mu_0$\item compute $Z = \frac{(\bar{x} - \mu_0)}{s}$ as before\item compute $p$-value as $p = 2 \Phi(Z)$ to account for both tails\end{itemize}[/latex]
[latex]General points about statistical hypothesis tests:\\- What if $p > \alpha$?\\- What are common assumptions in statistical tests?[/latex] 	[latex]\begin{itemize}\item What if $p > \alpha$?\\Failure to reject $H_0$; does not mean we accept $H_0$\item What are common assumptions in statistical tests?\\Every test makes some assumptions:\\$Z$-test and $t$-test assume normality, but generally data is far from normallydistributed (e.g. exponential runtime distributions)\end{itemize}[/latex]
The permutaation test: a better test for non-normal distributed data  	[latex]\begin{itemize}\item Framework for testing claims\item ex.: $H_0: X$ and $Y$ have equal means\item Test statistic: $t = \frac{1}{n} \sum \limits_{i=1}^{n}x_i - \frac{1}{m}\sum \limits_{j=1}^{m}y_j$\item Sampling distribution to compare $t$ against:\begin{itemize}\item put $x_1, \dots, x_n$ and $y_1, \dots, y_m$ into single pool\item $S = [];$ repeat, e.g. $10000$ times\\- draw random permutation and permute pool with it\\- add test statistic over permuted pool to $S$\end{itemize}\item $p$-value: fraction of samples $s$ in $S$ with $s < t$\end{itemize}[/latex]
Paired vs unpaired tests  	[latex]\begin{itemize}\item Data can come in pairs\item ex.: runtimes $x_i$ and $y_i$ for same instance\item paired statistical tests take pairing into account\item paired version of permutation test:\\- similar to unpaired version, but permute pools $\{x_i, y_i \} $ for all $i$\\- based on just first $10$ instances: $p = 0.0000$ (vs $0.073$ for rank-sum\end{itemize}[/latex]
Stochastic Gradient Descent (SGD) Properties  	Like Local search:[latex]\begin{itemize}\item Given loss function $\psi: \mathbb{R}^n \to \mathbb{R}$ for each datapoint in training set $\mathcal{D}$\item Loss across $\mathcal{D}$ is $f(x) := \mathbb{E}_{\psi \sim \mathcal{D}}[\psi(x)]$\item we locally follow gradient; learning rate determines step size\end{itemize}Stochastic gradient descent considers batches of data subsets\begin{itemize}\item draw batch $\mathcal{B}$ of data points from $\mathcal{D}$\item compute local gradients based only on $\mathcal{B}$\item randomness due to differences between $\mathcal{B}$ and $\mathcal{D}$\end{itemize}[/latex]
What does a learning curve reflect?  	Accuracy as a function of time
What's the most important hyperparameter of SGD?  	[latex]initial learning rate $\eta_0$[/latex]
Qualified runtime distributions: what do they reflect?  	Distributions of runtime to reach a specified solution quality
Performance Prediction - Idea and Targets  	Idea:- Observe empirical performance data- (probabilistically) predict other performanceTargets:- new parameter configurations- new problem instances- extrapolate: later time steps- extrapolate: larger data sets- combinations of all of these
Empirical Performance Models: Motivation  	-> Algo Selection-> Algo Configuration-> Study Algorithm Parameter Performance-> generating hard benchmarks-> gaining insights into instance hardness and algorithm performance
Empirical Performance Models (EPM) Intuition  	[latex]\begin{itemize}\item Given algorithm $A$ with possible configuration $\theta \in \Theta$\item Given instances described by features $z \in \mathcal{F}$\item EPMs = regression models $f: \Theta \times \mathcal{F} \to \mathbb{R}$\\$\Rightarrow$ EPMs also quantify uncertainty in their predictions\end{itemize}[/latex]
Empirical Performance Models (EPM): Definition  	[latex]\begin{itemize}\item Let $A$ be an algorithm with $k$ parameters $\theta_1, \dots, \theta_k$ wiTh domains $\Theta_1, \dots, \Theta_k$ and configuration space $\Theta =\Theta_1 \times \dots \times \Theta_k$\item Let problem instances $\pi \in \Pi$ be described by a list $z = [z_1,\dots, z_m]$ of features, drawn from a feature space $\mathcal{F}$\item input space is $\mathcal{I} = \theta \times \mathcal{F}$\item Let $\Delta(\mathbb{R})$ denote space of probability distributions over$\matbb{R}$\item An EMP $f$ is a stochastic process $f:\mathcal{I} \to\Delta(\mathbb{R})$; for each input, it outputs a probability distribution overperformance values\end{itemize}[/latex]
Extrapolating learning curves  	- Observe initial part of learning curve- probabilistically predict remainder of curve
Extrapolation Methods: Fitting parametric function  	[latex]\begin{itemize}\item increasing, saturating functions\item ex.: $pow_3$ function with $3$ parameters $c,a,\alpha$:\\$y = pow_3(x|c,a,\alpha) = c - ax^{-\alpha}$ (where $x = epoch, y = accuracy$)\item Fitting those parameters: optimize fit using gradient-based methods\end{itemize}[/latex]
Methods for extrapolation: Advanced variants  	-> No single parametric family performs best-> fit convex combinations of 11 different parametric distribution families=> In practice, would be good to have uncertainty estimates:-> Optimizing parameters yields single fit-> Sampling parameters yields distribution of fits=> Sample parameters according to posterior probability (e.g. using markovchain monte carlo)
