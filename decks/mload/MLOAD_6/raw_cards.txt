Good models (the scientific approach)  	-> empirically describe observed behaviour + permit testable predictions-> falsifiable (proving is never possible)
Elements of the scientific approach  	-> Create individual hypotheses and general models=> Create thinking; uses bold generalization-> design experiments=> some creativity, paired with knowledge how to analyze results-> analyse results=> has to handle unknown/uncontrollable factors==> statistical methodsAlternate between experimentation and hypotheses generation phases, iteratively imporve models
How does CS differ from other empirical sciences?  	-> object under study mathematically described-> complete control of object under study=> experiments reproducable perfectly-> cheap experiments=> computer prices decreasing=> often runtimes < 1 h (except deep learning up to week)-> conducting + analyzing experiments = fully automatically=> large amounts of data gatherable quickly==> However: statistical significance is not equal to relevance
Outliers for experiments in CS  	TODO: INSERT QUESTIONS+ANSWERS FROM SLIDES 9 + 10
Comparison: Manual algorithm analysis vs Automated algorithm design  	Manual algorithm analysis (descriptive):-> compare algorithm design A vs B-> identify types of instances where A outperforms B-> study robustness vs peak performanceAutomated Algorithm design (prescriptive):-> Exploit basic analysis as subroutine-> actively gather maximally informative data for that subroutine-> exploit areas of peak performance
Las Vegas Algorithms  	SLS + meta-heuristics (Las Vegas Algorithms) typically incomplete=> not guaranteed to find optimal solution in finite timeLas Vegas Algorithms - Properties for decision problems-> IF they return a solution = guaranteed correct -> runtime for finding solution subject to random variation
Definition: Generalized Las Vegas Algorithm (LVA) 	[latex]An algorithm $A$ for problem class $\Pi$ is a LVA iff:\begin{itemize}\item If $A$ terminates on problem instance $\pi \in \Pi$ with solution $s$, then $s$ is a correct solution of $\pi$\item The run-time of $A$ applied to any instance $\pi \in \Pi$ is a random variable $T_{A,\pi}$.\end{itemize}[/latex]
TODO: Questions for Examples of Las Vegas Algorithms (LVA) 	TODO: INSERT Q&A for SLIDE 15 SLIDESET 6
Application scenarios and evaluation criteria - What do they depend upon andwhich exist? 	[latex]Depend upon: application context\\ \\\begin{enumerate}\item No time limits\\- Algorithm run until solution is found\\- evalation criterion $=$ minimize expected run-time\item Hard time limit $t_{\mathit{max}}$\\- Solution found after $t_{\mathit{max}}$ = useless\\- evaluation criterion $=$ maximize solution probability at time $t_{\mathit{max}}$\item Characterized by utility function $U:R^+ \mapsto [0,1]$ with $U(t) $= utility of solution found at time $t$\\- decision-theoretic evaluation criterion $=$ utility-weighted solution  probability $U(t) \cdot P_s(T \leq t)$ (requires detailed knowledge of $P_s(T \leq t)\ \forall t$)\end{enumerate}[/latex]
Definition + Graph (for SLS algo): Runtime Distributions  	[latex]Given a LVA algorithm $A$ for a decision problem $\Pi$:\begin{itemize}\item $A$'s runtime on $\pi \in \Pi$ is a random variable $T_{A, \pi}$.\\$A$'s runtime-distribution on $\pi =$ distribution of $T_{A, \pi}$\item $A$'s success probability $P_s(T_{A, \pi} \leq t)$ until time $t$ iscumulative density function of runtime distribution evaluated at $t$\item runtime distribution function: $rtd(t) = P_s(T_{A, \pi} \leq t)$\end{itemize}[/latex]
What are empirical runtime-distributions?  	-> practical way to measure runtime-distribution-> approximations of true RTD=> based on N independent runs of algorithm=> runs = samples of theoretical RTD==> larger sample sizes N = more accurate approximations
[latex]Protocol for obtaining empirical RTD for LVA $A$ applied to given instance $\pi$ of a decision problem[/latex] 	[latex]\begin{enumerate}\item do $k$ independent ruuns of $A$ on $\pi$ with cutoff time$t_{\mathit{max}}$\\- generally: $k >= [50, 100]$\\- $t_{\mathit{max}}$ high enough to obtain large fraction of successful runs\item record runtimes of $k_{\mathit{success}}$ successful runs in list andsort according to increasing runtime\item Let $T(j)$ denote runtime from entry $j$ of sorted list\item Plot $(T(j), \frac{j}{k})$ = cumulative empirical RTD of $A$ on $\pi$\end{enumerate}[/latex]
Measuring runtimes  	-> CPU time measurement based on:- specific implementation of algorithm-  specific runtime environment=> to get reproducible and comparable results:- CPU times measured in way that is independent from machine load- runtime environment must be specified- implementation of algorithm made available==> measure runtimes using:-> operation counts (reflecting expensive steps)-> cost models = CPU time for each such operation
RTD-based analysis <- Application for LVA's  	basis to:-> analyze + characterize LVA behaviour-> compare performance of LVA's-> investigate impact of parameters, instance features etc on LVA's
Various graphical representations of run-time distributions  	TODO: INSERT PLOTS FROM SLIDES 28/29 SLIDESET 6
Asymptotic run-time behaviour of LVA's  	[latex]\begin{itemize}\item $A$ complete if finds solution in finite time for each soluble instance$\pi$\item $A$ is probably approximately complete (PAC) if for all soluble instances$\pi$: $\lim \limits_{t \to \infty} P_s(T_{A,\pi} \leq t) = 1$\item $A$ is essentially incomplete if there exists at least one solubleinstance $\pi$ with $\lim \limits_{t \to \infty} P_s (T_{A,\pi} \leq t) < 1$.\end{itemize}[/latex]
TODO: QUIZ FROM SLIDE 32 SLIDEST 6  	INSERT q&A from SLIDE 32 SLIDESET 6
The exponential distribution and LVA stagnation  	[latex]Exponential distribution:\begin{itemize}\item Let $X$ be random variable distributed to exponential distribution with parameter $\lambda$\\$\Rightarrow$ CDF of $X = P(X \leq x) = 1 - e^{-\lambda x}$\item CDF is memoryless:\\$P(X \leq s +t | X >s) = P(X\leq t)$\end{itemize}- If algorithms RTD is exponentially distributed, probability of success in  next $t$ steps is constant over time\\$\Rightarrow$ Stagnation of LVA detectable by comparing RTD against exponentialdistribution[/latex]
Performance improvements based on static restarts  	[latex]Static restarting:\\\begin{itemize}\item periodic reinitialization (every $t'$ time steps)\item helps overcome stagnation\end{itemize}To determine optimal cutoff-time for static restarts:\\\begin{enumerate}\item consider left-most exponential distribution that touches given empiricalRTD\item choose cut-off as smallest time value at which two respectivedistribution curves meet\end{enumerate}$\Rightarrow$ requires empirical RTD as known (or needs prediction)\\$\Rightarrow$ works only a posteriori\\$\Rightarrow$ optimal cutoff varies across instances[/latex]
Overcoming stagnation using dynamic restarts  	Dynamic restart strategies:-> Reinitialize process only when stagnation occurs-> needs to detect when stagnation occursSimple heuristic to detect stagnation:-> Incumbent did not improve for more than k steps-> k may depend on problem instance (e.g. on its size)-> k may increase over time
Main reason for SLS algorithm incompleteness; strategies to overcome  	-> Incompleteness due to inability of escaping attractive local minima=> random restarts, random walk, are strategies to overcome=> adaptive modification of parameters controlling search diversification also help (temperature in annealing, or tabu tenure in Tabu Search) 
