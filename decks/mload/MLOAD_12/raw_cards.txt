Name the two main components of Algorithm configuration! What are some general approaches for each? 	[latex]\begin{itemize}\item Component $1$: which configuration to choose? $=$ consider blackbox functionoptimization (abstract away complexity of evaluating multiple instances)\item Component $2$: how to evaluate a configuration?\end{itemize}General Approaches:\begin{itemize}\item Choosing configurations: Uniform Random Search, Gradient Descent and SLS,Population-based Search, Bayesian Optimization, Estimation of DistributionAlgorithms\item evaluating configurations: using fixed $N$ instances, Racing algorithms,Hoeffding races, Aggressive Racing, Adaptive Capping\end{itemize}[/latex]
Blackbox function optimization  	[latex]\begin{itemize}\item only mode of interaction: query $f(\theta)$ for an arbitrary $\theta \in\Theta$\item abstracts away complexity of evaluating multiple instances\end{itemize}[/latex]TODO INSERT IMAGE SLIDE 6 SLIDESET 12
Random Search to Choose Configurations  	[latex]Idea: Select configurations uniformly at random\begin{itemize}\item completely uninformed\item global search, no getting stuck in local minima\item at least better than grid search (especially if only few parametersmatter)\item parallelizable easily\end{itemize}[/latex]
Gradient Descent + Stochastic Local Search to choose configurations 	[latex]Trade off intensification and diversification\begin{itemize}\item intensification = Gradient descent\item diversification = restarts, random steps, perturbations, ...\end{itemize}Tabu search, SA, Iterated local search, ...[/latex]
Population-Based methods to choose configurations  	[latex]Population of configurations\begin{itemize}\item trade off global + local search\item maintain population fitness and diversity\end{itemize}[/latex]
Bayesian Optimization to choose configurations  	[latex]Approach:\begin{enumerate}\item Fit probabilistic model to collected function samples $\langle \theta,f(\theta) \rangle$\item use model to guide Optimization $=$ trade off exploration and exploitation\end{enumerate}Properties:\begin{enumerate}\item efficient in no. function evaluations\item robust: works when objective nonconvex, noisy, unknown derivatives, etc.\end{enumerate}[/latex]
Estimation of Distribution (EDA) Algorithms to choose configurations  	[latex]Approach:\begin{enumerate}\item Evaluate $f$ at random points, collect "good" points\item fit Distribution to good points\item iterate:\begin{itemize}\item sample points from current distribution, evaluate and collect good points\item refit distribution to good points\end{itemize}\end{enumerate}Relation to Bayesian optimization:\begin{enumerate}\item both probabilistic models\item both use model to inform where to evaluate next\item but models promising configurations: $P(\theta$ is good$)$ whereasBayesian optimization models $P(f|\theta)$\end{enumerate}[/latex]
Using Fixed N instances to evaluate configurations  	[latex]Treats algorithm configuration as blackbox function optimization problem (e.g.BasicILS(N))\\\\Issue: how large should $N$ be?\\\begin{itemize}\item too small $=$overtuning, too large = every function evaluation slow\end{itemize}General principle: don't waste time on bad configurations but evaluate goodconfigurations more in depth[/latex]
Racing algorithms to evaluate configurations  	[latex]Problem: which one of $N$ candidate algorithms is best?\\Idea:\begin{enumerate}\item Start with empty set of runs for each algorithm\item iterate\begin{itemize}\item perform one run each\item discard inferior candidates\item stop when single candidate remains or total configuration time cutoff reached\end{itemize}\end{enumerate}[/latex]
Hoeffding races to evaluate configurations  	[latex]\begin{enumerate}\item maintain confidence band for each algorithms performance\item drop algorithm if its confidence band doesn't overlap the best\end{enumerate}[/latex]TODO INSERT IMAGE SLIDE 16
Saving Time when evaluating configurations: Aggressive Racing  	[latex]\begin{enumerate}\item Race new configurations against current incumbent\begin{itemize}\item discard poor new configurations quickly\item statistical domination not required\item evaluate best configurations with many runs\end{itemize}\item search component should allow revisiting discarded configurations (mightjust have been unlucky)\end{enumerate}[/latex]
Saving time when evaluating configurations: Adaptive Capping  	[latex]when minimizing algorithm runtime, we can terminate runs for poorconfigurations $\theta'$ early: once wwe are guaranteed that $\theta'$ is worsethan $\theta$[/latex]
Tree Parzen Estimator  	[latex]Idea: Estimation of distribution\begin{enumerate}\item uses nonparametric density estimator to fit good points\begin{itemize}\item parzen estimator for each dimension\item $P(\theta$ is good$) = \prod \limits_{d=1}^{D} P($dimension $d$ of$\theta$ is good$)$\end{itemize}\item Tree of parzen estimators\begin{itemize}\item handles conditionality\item density estimates only computed for active dimensions\end{itemize}\item natively handles different parameter types (continuous, categorical, etc)\end{enumerate}Advantages:\begin{enumerate}\item human users can specify priors over good parameter ranges\item strict generalization of standard problem formulation (no priorswould equal uniform prior!)\end{enumerate}[/latex]
Spearmint and SMAC 	[latex]Approach:\begin{enumerate}\item Fit probabilistic model to collected function samples $\langle \theta, f(\theta) \rangle$\item use model to guide optimization $=$ trade off exploration vs exploitation\end{enumerate}design decisions\begin{itemize}\item which model family\item how to use model to guide optimization (acquisition function for datapoints)\end{itemize}[/latex]
