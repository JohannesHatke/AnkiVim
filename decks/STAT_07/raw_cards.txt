Classification  	[latex]\begin{itemize}    \item supervised learning    \item partitioning input space into decision           regions $=$ learning decision boundaries from data    \item interested in conditional probabilities $p(C_k |x)$, like regression, but $C_k$ is binary variable or vector(if multiclass)\end{itemize}[/latex]
Linear separability  	TODO: INSERT IMAGE FROM SLIDE 3 SLIDESET 07
Classification: Three classical approaches 	[latex]\begin{enumerate}    \item generative model: learn class-conditional densities $p(x|C_k)$ aswell as prior probabilities $p(C_k)$ and infer posterior using Bayes:\begin{equation*}p(C_k|x) = \frac{p(x|C_k) p(C_k)}{p(x)}\end{equation*}\end{enumerate}    \item discriminative model: directly learn conditional $p(C_k|x)$, mostsimilar to regression    \item learn decision boundary directly, ignore probabilities[/latex]
Linear discriminant functions  	[latex]for two-classes:\begin{enumerate}    \item linear function form:\begin{equation*}y(x) = w^\top x + w_0\end{equation*}    \item decision boundary given by $y(x) = 0  \Rightarrow$ if $y(x)$ positive, x assigned to $C_1$ else $C_2$    \item weight vector $w$ orthogonal to decision boundary    \item signed distance of point to hyperplane obtained by$\frac{y(x)}{||w||}$    \item multiple classes cause problems when combinng multiple two-classdiscriminant functions $=$ use single discriminant with multiple functionsinstead\end{enumerate}[/latex]
Multiple classes: Classification  	[latex]cause problems when combining multiple two-class disccriminant functions\begin{enumerate}    \item avoided by single disccriminant consisting of multiple linearfunctions:\begin{align*}y_k(x) &= w_{k}^{\top}x + w_0\\C_k = 1 &\iff y_k(x) = \max \limits_j y_j(x)\end{align*}    \item decision boundary for each pair given by:\begin{equation*}y_k(x) = y_j(x)\end{equation*}or\begin{equation*}(w_k - w_j)^\top x + w_{k0} - w_{j0} = 0\end{equation*}    \item decision regions are convex\end{enumerate}[/latex]
Classification: Linear discriminant - Estimating weights  	[latex]\begin{itemize}    \item building classifier $=$ Estimating weights from data:    \item let: $y(x) = \bar{w}^\top \bar{x}$ where $\bar{w} =(w_0,w^\top)^\top, \bar{x} = (1, x^\top)^\top$        \item least squares approach $=$ minimize sum of squares error:\begin{equation*}E(\bar{w}) = \frac{1}{2} (X\bar{w}-T)^\top (X\bar{w}-T)\end{equation*}        which gives:\begin{equation*}\bar{w} = (X^\top X)^{-1} X^\top T\end{equation*}where $X^\top X)^{-1} X^\top$ is pseudo-inverse of $X$\end{itemize}[/latex]
Fisher's linear discriminant  	[latex]\begin{enumerate}    \item other way to interpret linear discriminant $=$ subspace projection.Project inputs $x$ down to one-dimensional space + apply threshold toseperate classes:\begin{equation*}y =w^\top x\end{equation*}    \item direction of projection given by $w$    \item we get:\begin{equation*}w \propto S_{W}^{-1}(m_2 - m_1)\end{equation*}where $m_2, m_1$ sample means and:\begin{equation*}S_W = \sum \limits_{n \in C_1}(x_n -m_1) (x_n -m_1)^\top + \sum \limits_{n \in C_2}(x_n -m_2) (x_n -m_2)^\top \end{equation*}    \item equivalent to least squares up to missing bias $w_0$\end{enumerate}[/latex]
The perceptron algorithm  	[latex]\begin{enumerate}    \item perceptron:\begin{equation*}y(x) = f(w^\top x)\end{equation*}where $f(a) = \begin{cases} +1, &a >= 0\\-1, &a < 0\end{cases}$     \item seeks to minimize:    \begin{equation*}E(w) = - \sum \limits_{n \in M}w^\top x_n t_n    \end{equation*}where $t_n =$ target values (-1 or +1)        \item done by gradient-descent algorithm (converges if dataset linearlyseparable):\begin{equation*}w^{k+1} = w^k + r x_n t_n\end{equation*}\end{enumerate}[/latex]
Boosting: General Idea 	[latex]\begin{enumerate}    \item use comittee instead of single classifier    \item each base/weak classifier can be simple and perform poorly, weightedaverage of classifiers performs well    \item train classifiers in sequence and each trainining step involves newweighting of data $=$ misclassified data points get higher weight in nextiteration\end{enumerate}[/latex]
Boosting  	[latex]\begin{enumerate}    \item initialize weights equally: $w_{n}^{1} = \frac{1}{N}$    \item fit base classifier $y_m(x)$ to training data $=$ minimize:\begin{equation*}\sum \limits_{n=1}^{N} w_{n}^{m} \delta(y__m(x_n) \neq t_n)\end{equation*}    \item evaluate:\begin{equation*}\epsilon_m = \frac{\sum \limits_{n=1}^{N} w_{n}^{m} \delta(y_m(x_n) \neq t_n)}{\sum \limits_{n=1}^{N}w_{n}^{m}}\end{equation*}and\begin{equation*}\alpha_m = ln(\frac{1-\epsilon_m}{\epsilon_m})\end{equation*}    \item update sample weights:\begin{equation*}w_{n}^{m+1} = w_{n}^{m} exp(\alpha_m \delta(y_m(x_n) \neq t_n)\end{equation*}    \item final model given by:    \begin{equation*}y(x) = sign(\sum \limits_{m=1}^{M} \alpha_m y_m(x))    \end{equation*}\end{enumerate}[/latex]
Probabilistic models for classification 	[latex]\begin{enumerate}    \item Motivation: directly learning decision function $=$ no probabilistic treatment ofdata, no inference and no quantification of uncertainty of decision    \item generative or discriminative models exist\end{enumerate}[/latex]
Classification with generative model  	[latex]generative $=$ model likelihood $p(x|C_k)$, prior $p(C_k)$ + generate/derive posterior $p(C_k|x)$ with Bayes\begin{enumerate}    \item simple model for $p(x|C_k) = $ Gaussian with shared covariance:\begin{equation*}p(x|C_k) = \frac{1}{|2\pi \Sigma|^{1/2}} exp(-\frac{1}{2} (x-\mu_k)^\top\Sigma^{-1}(x-\mu_k)\end{equation*}    \item learn parameters using maximum likelihood:\begin{align*}\mu_k &= \frac{1}{N_k} \sum \limits_{n=1}^{N} t_{nk}x_n \\\Sigma &= \frac{1}{N} \sum \limits_{k=1}^{K} \sum \limits_{n=1}^{N} t_{nk} (x_n- \mu_k)(x_n - \mu_k)^\top\end{align*}where $t_{nk} = 1$ iff sample $n$ belongs to class $k$ else $0$    \item like Gaussian mixture, but we know class labels!    \item maximum likelihood also gives prior:\begin{equation*}p(C_k) = \frac{1}{N} \sum \limits_{n=1}^{N} t_{nk} = \frac{N_k}{N}\end{equation*}    \item Decision in generative model: choose class with largest probability:\begin{equation*}C_k(x) = argmax_k p(C_k|x)\end{equation*}    \item gaussians with shared covariance $=$ linear decision boundary;flexible covariance $=$ quadratic boundaries (can be non-connected)\end{enumerate}[/latex]
Discriminative models for classification  	[latex]\begin{enumerate}    \item directly estimate parameters of posterior $p(C_k|x)$ withoutestimating possibly complex distribution $p(x|C_k)$    \item two-class case:\begin{equation*}p(C_1|x) = \sigma(w^\top x + w_0)\end{equation*}where $\sigma(a) = \frac{1}{1+exp(-a)$ is logistic sigmoid function.\end{enumerate}    \item sigmoid can be viewed as smoothed step function that maps real valuesto values in $[0,1]$ (outcome is probability)[/latex]
Motivation for sigmoid function  	[latex]\begin{itemize}    \item Regarding generative model we have:\begin{equation*}p(C_1|x) = \frac{p(x|C_1) p(C_1)}{p(x|C_1) p(C_1) + p(x|C_2) p(C_2)} =\frac{1}{1+exp(-a)} = \sigma(a)\end{equation*}where $a = ln \frac{p(x|C_1) p(C_1)}{p(x|C_2) p(C_2)}$which is the log-odds ratio of probabilities    \item linear model $a = w^\top x + w_0$ corresponds to log odds of Gaussiandistribution with shared covariance; we have:\begin{align*}w &= \Sigma^{-1}(\mu_1 - \mu_2) \ (\text{Fishers linear discriminant})\\w_0 &= -\frac{1}{2} \mu_{1}^\top \Sigma^{-1} \mu_1 + \frac{1}{2} \mu_{2}^\top\Sigma^{-1} \mu_2 + ln \frac{p(C_1)}{p(C_2)}\end{align*}\end{itemize}[/latex]
Logistic regression (Classification - Discriminative model)  	[latex]\begin{enumerate}    \item learn weights directly (instead of parameters of Gaussian -generative model)    \item if we further allow nonlinear transformation of input to featurespace, get similar problem to regression with logistic sigmoid (logisticregression):\begin{equation*}p(C_1|x) = \sigma(w^\top \phi(x))\end{equation*}    \item advantage of transformation: inputs must not be linearly seperable ininput space, only in feature space    \item with complex high-dimensional feature space, more datasets becomeseperable; but risk: over-fitting\end{enumerate}[/latex]
Logistic regression algorithm (Classification - discriminative model)  	[latex]\begin{enumerate}    \item like regression: optimize weights by minimizing negative log-likelihood:\begin{equation*}E(w) = -ln(p(T|w)) = - \sum \limits_{n=1}^{N} (t_n\ ln(y_n) + (1-t_n)\ln(1-y_n))\end{equation*}where $y_n = \sigma(w^\top \phi(x_n))$\item derivative of sigmoid is:\begin{equation*}\frac{d\sigma(a)}{da} = \sigma(a) (1-\sigma(a))\end{equation*}\item .. therefore gradient is:\begin{equation*}\frac{\delta E(w)}{\delta w} = \sum \limits_{n=1}^{N} (y_n - t_n) \phi(x_n)=\sum \limits_{n=1}^{N} (\sigma(w^\top \phi(x_n)) -t_n)\phi(x_n)\end{equation*}\item unlike regression: gradient nonlinear in $w$ due to sigmoid $\rightarrow$no closed form solution\item use iterative Newton-Raphson method to find solution:\begin{equation*}w^{new} = w^{old} - H^{-1} \Delta E(w)\end{equation*}where $H=\Delta \Delta E(w)$ is Hessian of cost function.\item we have:\begin{align*}\Delta E(w) &= \frac{\delta E(w)}{\delta w} = \sum \limits_{n=1}^{N} (y_n -t_n)\phi(x_n) = \Phi^\top (Y-T)\\H = \sum \limits_{n=1}^{N} y_n (1-y_n)\phi(x_n) \phi(x_n)^\top = \Phi^\top R\Phi\end{align*}\item Hessian is positive definite $\rightarrow$ cost function convex withsingle optimum\end{enumerate}[/latex]
Logistic regression: multiple classes (Classification - discriminative model)  	[latex]\begin{enumerate}    \item counterpart to sigmoid function for multiple classes: normalizedexponential:\begin{equation*}p(C_k|x) = \frac{p(x|C_k) p(C_k}{\sum \limits_j p(x|C_j) p(C_j)} =\frac{exp(a_k)}{\sum \limits_j exp(a_j)}\end{equation*}\item it depends on activations:\begin{equation*}a_k = w^\top \phi(x)\end{equation*}\item negative log-likelihood is:\begin{equation*}E(w_1, \dots, w_k) = -\sum \limits_{n=1}^{N}\limits_{k=1}^{K} t_{nk}\ ln(y_k)\end{equation*}where $y_k = p(C_k|x_n)$\item we get following gradient and block components of Hessian:\begin{align*}\delta w_j E(w_1, \dots, w_K) &= \sum \limits_{n=1}^{N} (y_{nj}-t_{nj})\phi(x_n)\\\delta w_k \delta w_j E(w_1, \dots, w_K) &= \sum \limits_{n=1}^{N} y_{nk}(y_{nj}-\delta_{kj})\phi(x_n)\phi(x_n)^\top\\\end{align*}\item we can use these for Newton-Raphson method:\begin{equation*}w^{new} = w^{old} - H^{-1} \Delta E(w)\end{equation*}\item here: $w$ $K\cdot M$ dimensional vector and Hessian $K\cdot M \times K\cdot M$ matrix.\end{enumerate}[/latex]
Bayesian logistic regression (Classification - discriminative model)  	[latex]Question: can we introduce prior on weights like in regression?\begin{enumerate}    \item due to nonlinear sigmid function, there is no conjugate prior thatallows exact evaluation of:\begin{equation*}p(C_1|, T) = \int p(C_1|\phi(x), w) p(w|T) dw\end{equation*}\item approximate solutions can be found: use Laplace approximation ofnon-Gaussian distribution by Gaussian one\item this gives: Gaussian posterior $p(w|T)$ and results in convolution ofsigmoid with Gaussian:\begin{equation*}p(C_1|X, T) \appro \int \sigma(a) \mathcal{N}(a|\mu_a, \sigma_{a}^{2}) da\end{equation*}\item with different approximation of sigmoid by \emph{probit} function, result canalso be expressed analytically\end{enumerate}[/latex]
