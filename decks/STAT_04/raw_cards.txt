Nonparametric methods: Motivation  	[latex]\begin{enumerate}\item parametric density models try representing distribution by fewparameters (estimated from training samples)\\$\Rightarrow$ often too restricted in distributions they can represent\\$\Rightarrow$ mixture models overcome this, but require complex training(EM-algo)\item Idea: Nonparametric models $=$ parameters are trivially training samplesthemselves\item Advantage: not restricted w.r.t densities they can represent\\Disadvantage: high memory + computational demands at test/predict time\end{enumerate}[/latex]
Histograms  	\begin{enumerate}\item binned histogram $=$ basic nonparametric model \item divide space into several bins of (usually equal) width $\delta_i$\item counting number $n_i$ of samples falling into bin $i$ and obtain binprobabilities:\begin{equation*}p_i = \frac{n_i}{N \delta_i}\end{equation*}\item integration over histogram bins gives:\begin{equation*}\int p(x) dx = \sum \limits_i p_i \delta_i = 1\end{equation*}\end{enumerate}
Histograms - Bin width properties 	[latex]\begin{enumerate}\item bin width $\Delta_i$ has strong impact on estimated density\item small bins $=$ capture details of density but overfitting (few samplesper bin)\item large bins $=$ sufficient samples in each bin to avoid overfitting, butlocal details lost\item observation: many samples are good\item boundaries between bins introduced arbitrarily\\$\Rightarrow$ samples near boundary of two bins contribute to only single bin\\$\Rightarrow$ better $=$ weighted counting\end{enumerate}[/latex]
Histograms: Advantages and Disadvantages  	[latex]\begin{enumerate}\item advantage: nonparametric, but only $n_i$ must be stored, trainingssamples can be thrown away\item disadvantage: suffer extremely from curse of dimensionality.\\Size of histogram grows exponentially with dimensionality $D$\item disadvantage: corresponding memory requirements (stem from exponentialgrowth) prohibit histograms for $D \geq 4$. \end{enumerate}[/latex]INSERT IMAGES FROM SLIDE 5
Density Estimation: General Ideas  	[latex]\begin{itemize}\item estimate local density $p(x) = \frac{K}{N \cdot V}$, where:\begin{enumerate}\item $K = $ number of samples in neighbourhood\item $V = $ volume of neighbourhood\item $N = $ total number of samples\end{enumerate}\item fixed surrounding $=$ kernel density estimator\item fixed number of samples in neighbourhood (fixed $K)=$ k-nearest neighbours\end{itemize}[/latex]
Kernel density estimator: General Idea  	[latex]\begin{enumerate}\item instead of fixing bins and counting samples in bins, fix the samples andsum kernels (similar to bins) centered at the samples\item shape of "bins" described by kernel function $k(y)$\item histogram bins $=$ box-shaped kernels:\begin{equation*}k(y) = \begin{cases}\Delta^{-D} &\mbox{ if } |y_i| \leq \frac{\Delta}{2} \forall i \in \{1, \dots,D\}\\0 &\mbox{ else }\end{cases}\end{equation*}\item smoother kernels, such as Gaussian kernel can be used:\begin{equation*}k(y) = \frac{1}{2 \pi h^2}^{\frac{D}{2}} \exp (- \frac{||y||^2}{2h^2})\end{equation*}\end{enumerate}[/latex]
Kernel density estimator/Parzen estimator: Using the kernel  	[latex]\begin{enumerate}\item density can be estimated as sum of kernel functions normalized by theirnumber (this called parzen estimator)\begin{equation*}p(x) = \frac{1}{N} \sum \limits_{n=1}^{N} k(x-x_n)\end{equation*}\item if kernel is a density, then average of kernels is also density. Thus,requirements for kernel function are:\begin{equation*}k(y) \geq 0, \ \ \ \int k(y) dy = 1\end{equation*}\item kernel should be sufficiently smooth to yield smooth density\item kernel acts on distance of two points, hence we often write:\begin{equation*}k(x_1, x_2) = k(x_2 - x_1)\end{equation*}\end{enumerate}[/latex]
Kernel density estimator: Properties  	[latex]\begin{enumerate}\item kernel density estimator can be seens as limiting case of mixturedistribution where each sample is its own mixture component\\ \\Example: Gaussian kernel density estimator and Gaussian mixture:\begin{equation*}p(x) = \frac{1}{N} \sum \limits_{n=1}^{N} k(x-x_n) = \sum \limits_{n=1}^{N}\frac{1}{N} \mathcal{N}(x|x_n, h)\end{equation*}\item advantages kernel density estimator:\begin{itemize}\item no em required, no local maxima\item highest accuracy\item only one hyperparameter $h$\end{itemize}\item disadvantages kernel density estimator:\begin{itemize}\item all training samples must be stored and processed to compute density\item no local adaption of $h$\end{itemize}\end{enumerate}[/latex]
How to determine a good value for kernel width parameter $h$ (e.g. in kerneldensity estimation)? 	Cross-validation.
Cross-validation: General Idea + Dilemma 	[latex]\begin{enumerate}\item general way to estimate hyperparameters or model parameters (number ofmixture coefficients of Gaussian mixture)\item Idea: choose parameter so that average performance on test set ismaximized)\item training data is rare $\Rightarrow$ dilemma: want large training set forlearning and large test set to estimate average performance\end{enumerate}[/latex]
Cross-validation: Algorithm  	[latex]\begin{enumerate}\item split training data into $M$ parts\item use one part for testing and rest for learning\item gives $M$ different combinations of learning and testing\end{enumerate}Case $M=N$ is called leave-one-out cross-validation.[/latex]
Cross-validation: What is a problem and why?  	[latex]problem $=$ dataset contains duplets. [/latex]TODO: LOOK AT LECTURE 04 AND FIND OUT WHY THIS IS A PROBLEM!
Cross-validation and kernel density estimator  	[latex]we need loss function to quantify performance (cross-validation)\begin{enumerate}\item reasonable choice $=$ maimize likelihood of samples not used fortraining, for leave-one-out CV, this is a single sample:\begin{equation*}\mathcal{L}(h) = ln (\hat{p}_{i,h}(x_i)) = ln(\frac{1}{N-1} \sum \limits_{n=1,n \neq i}^{N} k_h(x_i, x_n))\end{equation*}\item we wish to maximize expectation of this loss function:\begin{equation*}\mathbb{E}[\mathcal{L}(h)] = \frac{1}{N} \sum \limits_{i=1}^{N}ln(\hat{p}_{i,h}(x_i))\end{equation*}\item can be done either by direct search methods or gradient descent\item alternative loss function $=$ integrated squared error (betterconvergence properties):\begin{equation*}\mathcal{L}(h) = \int (p(x) - \hat{p}_{i,h}(x))^2\ dx\end{equation*}\end{enumerate}[/latex]
Convergence of kernel density estimator  	[latex]\begin{enumerate}\item can be shown that kernel density estimator converges to true density for$N\to \infty$ (unbiased estimator)\item requires that kernel width decreases appropriately with increasing numberof samples\item cross-validation with integrated squared error loss function assuresconvergence\item cross-validation with maximum likelihood loss function has problems withkernels with infite tails (e.g. Gaussian). Other kernels like Epanechnikovkernel assure convergence.\item rules of thumb for choosing $h$ exist\end{enumerate}[/latex]
Nearest-neighbor methods: General Idea  	[latex]\begin{enumerate}\item parameter $K$ determines amount of smoothing\item can be used for classification\item opposite to kernel density estimator, estimate is noisy in dense regions(compromise $=$ adaptive kernel densities)\item likelihood of sample given class can be estimated as:\begin{equation*}p(x|\mathcal{C}_i) = \frac{K_i}{N_i V(x)}\end{equation*}\item marginal density is $p(x) = \frac{K}{NV(x)}$\item class priors: $p(\mathcal{C}_i) = \frac{N_i}{N}$\item bayes formula yields posterior:\begin{equation*}p(\mathcal{C}_i |x) = \frac{p(x|\mathcal{C}_i)p(\mathcal{C}_i)}{p(x)} =\frac{K_i}{K}\end{equation*}\item for new point $x$ only look at $K$ nearest neighbors in training set andassign most frequent class\end{enumerate}[/latex]
Adaptive kernel density estimator: Motivation  	[latex]\begin{enumerate}\item kernel density estimator leads to noisy estimates in sparse regions\item k-NN estimator is noisy in dense regions\item Idea: employ nearest neighbor rule to estimate locally adaptive kernelwidth $h(x)$\end{enumerate}[/latex]
 Adaptive kernel density estimator: Steps 	[latex]Idea: employ nearest neighbor rule to estimate locally adaptive kernelwidth $h(x)$\begin{enumerate}\item compute variance of $K$ nearest neighbors:\begin{equation*}\sigma^2(x) = \frac{1}{K} \sum \limits_{i=1}{K}||x_i - \mu||^2\end{equation*}where $\mu = \frac{1}{K}\sum \limits_{i=1}^{K}x_i$\item set kernel width to $h(x) = \sigma(x)$ and estimate density as:\begin{equation*}p(x) = \frac{1}{N} \sum \limits_{n=1}^{N}k_{h(x)}(x, x_n)$\end{equation*}\item hyperparameter $K$ estimated via cross-validation\end{enumerate}[/latex]
Anisotropic kernel density estimator  	[latex]\begin{enumerate}\item Idea: extend adaptivity idea to multivariate data by using anisotropickernels\item from $K$ nearest neighbors estimate covariance matrix\begin{equation*}\Sigma(x) = \frac{1}{K} \sum \limits_{i=1}{K}(x_i - \mu)(x_i -\mu)^\top\end{equation*}where $\mu = \frac{1}{K}\sum \limits_{i=1}^{K}x_i$\item estimate density:\begin{equation*}p(x) = \frac{1}{N |2\pi \Sigma(x)|^{\frac{1}{2}}} \sum \limits_{n=1}^{N} \exp (-\frac{1}{2}(x_n -x) \Sigma^{-1}(x)(x_n-x)^\top)\end{equation*}\item anisotropic kernels better adapt to structure of density\end{enumerate}[/latex]
Approximate nearest neighbor methods  	[latex]\begin{enumerate}\item main drawback of nonparametric methods $=$ no training phase wouldarrange data for fast decisions afterwards\\ $\Rightarrow$ computational cost at time ofdecision grows linearly with total number of samples $N$\item but: most of data does not matter for certain decision\begin{itemize}\item only $K$ nearest neighbors play role in $k$-NN\item most samples too far from point $x$ to contribute to kernel density $p(x)$\end{itemize}\item there are ways to arrange data for access to required subset in sublineartime\item in contrast to other learning techniques that reduce amount of data, thiseven increases memory consumption\item in higher dimensions, the techniques can only ensure approximate proximity\end{enumerate}[/latex]
K-d trees: General Idea 	[latex]\begin{enumerate}\item fast nearest neighbor methods: based on subdividing space into buckets.\item Given query, finding corresponding (most similar subset) bucket can be done in sublinear time\item simplest strategy for bucket division (clustering) is to recursivelysubdivide space along coordinate axes\item pointers to subsets of samples can be arranged in tree structure: k-d tree\item varying splitting criteria possible: usually cycle all axes and splits.t. both sides contain same number of samples (prefer balanced trees)\end{enumerate}[/latex]
K-d trees: Steps  	[latex]\begin{enumerate}\item search for nearest subset to query $=$ sublinear time by descending tree\item subset can be used to find $K$ nearest neighbors or to estimate localdensity\item far away samples are neglected (pruning)\item drawback: some close samples also neglected (in particular ifsamples/queries close to decision boundary)\\$\Rightarrow$ looks negligible in $2$D, but more severe in higher dimensions\end{enumerate}[/latex]
K-d tree: reducing number of missed samples  	[latex]Different methods exist:\begin{enumerate}\item prune parts of tree only if sure they cannot contain one of $K$ nearestneighbors (exact nearest neighbors)\\$\Rightarrow$ in high-dimensional spaces, this results in searching whole tree\item live with approximate set of nearest neighbors, but increase accuracy ofapproximation:\begin{itemize}\item splitting not only along coordinate axes\item spill trees\item multiple randomized trees\end{itemize}\end{enumerate}[/latex]
Tree-structured vector quantization  	[latex]Idea: Tree built by splitting along arbitrary axes (instead of coordinate axes)\begin{enumerate}    \item recursive splitting$=$ k-means clustering, for binary tree $K=2$    \item adapts better to data in high-dimensional spaces    \item no severe additional costs:    \begin{itemize}        \item costs for building tree $O(N log N)$        \item costs for querying tree $O(log N)$    \end{itemize}    \item splitting along arbitary axes yields tree-structed vectorquantization (TSVQ) also called cluster tree\end{enumerate}[/latex]
Spill trees  	[latex]\begin{itemize}    \item extension of either k-d trees or cluster trees\end{itemize}General Idea: introduce overlapping zone of width $w$ around decisionboundaries and assign samples in overlapping zone to both buckets\begin{enumerate}    \item alleviates problem of samples near decision boundaries    \item increases memory requirements dramatically if $w$ chosen too large    \item well-suited for density estimation with Epanechnikov kernel and fixed    kernel width. Exact solution is obtained for $w \geq h$\end{enumerate}[/latex]
Randomized Trees  	[latex]\begin{itemize}    \item another idea to deal with missed sample problem (samples inoverlapping regions between buckets) $=$ use multiple trees and randomize theirconstruction\end{itemize}\begin{enumerate}    \item most successful in practice    \item union of all points contains exact $k$-nearest neighbors with high    probability    \item similar method is known as locality sensitive hashing (LSH)    \item works of Indyk et al. contain theorems on probability for finding    exact $k$-nearest neighbors    \item Generally, if accuracy goes up, efficiency goes down $=$ tradeoff\end{enumerate}[/latex]
