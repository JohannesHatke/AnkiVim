What is regression?  	[latex]\begin{enumerate}    \item goal $=$ learn predictive distribution $p(t|x)$ of continuous output    variables $t$ given continuous inputs $x$    \item general form of fitting parametric function through number of    input-output pairs including uncertainty of the fit    \item supervised learning: training set consists of input-output pairs!    \item probabilistic version of interpolation/approximation\end{enumerate}[/latex]
Basis functions  	[latex]\begin{enumerate}    \item Linear regression models represent sought function as linearcombination of basis function $\phi_j(x)$:\begin{equation*}y(x,w) = \sum \limits_{j=0}^{M-1} w_j \phi_j(x) = w^\top \phi(x)\end{equation*}\item examples for basis functions: polynomials, splines (local polynomials), radial basis functions, fourier basis, \item choice of basis function $=$ application/sought-function dependent\item machine learning part mostly independent of choice of basis function\end{enumerate}[/latex]
Gaussian noise  	[latex]\begin{enumerate}    \item probabilistic nature of regression $=$ main difference between interpolationa and regression    \item Assume target variable $t$ given as deterministic function $y(x,w)$plus additive Gaussian noise:\begin{equation*}t = y(x,w) + \eta\end{equation*}\item described by conditional distribution ($\beta$: precision = inversevariance)\begin{equation*}p(t|x, w, \beta^{-1}) = \mathcal{N}(t|y, \beta^{-1}), \ \beta =\frac{1}{\sigma^2}\end{equation*}\item ...gives expectation:\begin{equation*}\mathbb{E}[t|x] = \int t_p(t|x, w)dt = y(x,w)\end{equation*}\item note that $y$ denotes mean prediction we estimate together withdistribution. In constrast, $t$, is observable target random variable includingnoise.\end{enumerate}[/latex]
Maximum Likelihood of conditional Gaussian  	[latex]$X, T$ vector of inputs/outputs of length $N$. ML estimation:\begin{align*}\log p(T|X, w, \beta) = \frac{N}{2} \log \beta - \frac{N}{2} \log(2 \pi) -\frac{\beta}{2} \sum \limits_{n=1}^{N} (t_n-w^\top \phi(x_n))^2\end{align*}Gradient of estimation term with respect to $w =$ linear system:\begin{equation*}\frac{\delta \log p(T|X,w, \beta)}{\delta w} = \beta \sum \limits_{n=1}^{N}(t_n -w^\top \phi(x_n)) \phi(x_n) = 0\end{equation*}Typical result of least squares problem: $T = \Phi \cdot w$:\begin{equation*}w_{ML} = (\Phi^\top \Phi)^{-1} \Phi^\top T = \Phi^\dagger T\end{equation*}where $\Phi$ is the non-quadratic matrix:[/latex]INSERT IMAGE OF MATRIX PHI FROM SLIDE 6
Pseudo-Inverse: Formula + General Idea 	[latex]Formula:\begin{equation*}\Phi^\dagger = (\Phi^\top \Phi)^{-1} \Phi^\top\end{equation*}where $\Phi$ is nonquadratic matrix.\\\begin{itemize}    \item yields minimum squared error solution of over-determined linear systemof equations (that has no exact solution)\end{itemize}[/latex]
Computing the Pseudo-Inverse: Different ways 	[latex]\begin{itemize}    \item singular value decomposition (SVD). Pseudo-inverse of diagonal matrix$\Sigma$: invert all non-zero entries and transpose     \item householder method: directly compute least squares solution to system:    \begin{equation*}    \Phi w_{ML} = T    \end{equation*}\end{itemize}Note: weight estimate is independent of precision $\beta$.\\ \\Noise precision can be estimated in ML framework via:\begin{equation*}\frac{1}{\beta_{ML}} = \frac{1}{N} \sum \limits_{n=1}^{N} (t_n - w_{ML}^{\top}\phi(x_n))^2\end{equation*}[/latex]
Multiple Output Regression  	[latex]\begin{enumerate}    \item multiple outputs $=$ vector-valued target:\begin{equation*}y(x,w) = W^\top \phi(x)\end{equation*}\item assume Gaussian noise with precision $\Lambda$ yields likelihood:\begin{equation*}p(T|X,x,W, \Lambda) = \mathcal{N}(T|W^\top \phi(x), \Lambda^{-1})\end{equation*}\item ML result is: $W_{ML} = \Phi^\dagger T$\item mean prediction for target channels(indices) are independent since ML estimate ofmean is independent of covariance\item channel correlations only play a role in estimated covariance:\begin{equation*}\Sigma = \Lambda^{-1} = \frac{1}{N} \sum \limits_{n=1}^{N} (t_n - W^\top\phi(x_n)) (t_n -W^\top \phi(x_n))^\top\end{equation*}\end{enumerate}[/latex]
Bayesian regression: General Idea 	[latex]\begin{enumerate}    \item complicated model with many basis functions $=$ ML overfitting    \item to cope with this maximize posterior distribution instead including          reasonable prior    \item conjugate prior of gaussian is gaussian:\begin{equation*}p(w|m_0, S_{0}^{-1}) = \mathcal{N}(w|m_0, S_{0}^{-1})\end{equation*}    \item to reduce number of hyperparameters, use special case of isotropicgaussian with zero mean:\begin{equation*}p(w|\alpha) = \mathcal{N}(w|0, \alpha^{-1} I)\end{equation*}\end{enumerate}[/latex]
Bayesian Regression: Maximizing posterior (computation)  	[latex]\begin{itemize}    \item Use isotropic, zero mean Gaussian: $p(w|\alpha) = \mathcal{N}(w|0,\alpha^{-1}I)$    \item conjugate prior also Gaussian: $p(w|m_0, S_{0}^{-1}) =\mathcal{N}(w|m_0, S_{0}^{-1})$    \item posterior is again Gaussian:\begin{equation*}p(w|T, X) \propto p(w|X, w) p(w|\alpha) = \mathcal{N}(w|m_N, S_{N}^{-1})\end{equation*}\item maximizing posterior:\begin{equation*}\log p(w|T,X) = -\frac{\beta}{2} \sum \limits_{n=1}^{N} (t_n - w^\top\phi(x_n))^2 - \frac{\alpha}{2} w^\top w + const\end{equation*}\item ... yields parameters:\begin{align*}m_N &= \beta S_{N}^{-1} \Phi^\top T\\S_N &= \alpha I + \beta \Phi^\top \Phi\end{align*}(for $\alpha \to 0$ we obtain $m_N = (\Phi^\top \Phi)^{-1} \Phi^\top T$)\item second term in posterior ($-\frac{\alpha}{2}\dots$) stems from prior $+$ iscalled regularizer. It shrinks parameters and so enforces simple solutions evenwith many basis functions. This regularized linear regression model is calledridge regression.\end{itemize}[/latex]
Sparse models: the lasso  	[latex]\begin{enumerate}    \item conjugate gaussian prior in regression shrinks weights a little butnot to zero    \item Laplace prior $=$ zero weights can be obtained:\begin{equation*}p(w|\alpha) = (\frac{\alpha}{4})^M \exp(-\frac{\alpha}{2} \sum\limits_{j=1}^{M} |w_j|)\end{equation*}    \item posterior distribution in this case:    \begin{equation*}    \log p(w|T,X) = -\frac{\beta}{2} \sum \limits_{n=1}^{N} (t_n - w^\top\phi(x_n))^2 - \frac{\alpha}{2} \sum \limits_{j=1}^{M} |w_j| + const    \end{equation*}\item drawback: laplace not conjugate to gaussian likelihood $\Rightarrow$ noclosed-form solution, maximizing posterior gives nonlinear system\end{enumerate}[/latex]
Predictive distribution  	[latex]\begin{enumerate}    \item not only interested in weights $w$ but also in predictivedistribution of target:\begin{equation*}p(t|T, X, x, \alpha, \beta) = \int p(t|x,w,\beta) p(w|T,X,x,\alpha, \beta)dw\end{equation*}\item both distributions are gaussian, so result is too:\begin{equation*}p(t|T,X,x,\alpha, \beta) = \mathcal{N}(t|m_{N}^{\top} \phi(x),\sigma_{N}^{2}(x))\end{equation*}\item regression different from interpolation - we get mean prediction:\begin{equation*}y(x) = m_{N}^{\top} \phi(x)\end{equation*}and variance:\begin{equation*}\sigma_{N}^{2}(x) = \frac{1}{\beta} + \phi(x)^\top S_{N}^{-1}\phi(x)\end{equation*}\item variance represents uncertainty of predition, consists of noise variance$\beta^{-1}$ and uncertainty in parameters $w$. Parameter uncertainty shrinksto zero as dataset size grows.\end{enumerate}[/latex]
Model Selection  	[latex]\begin{enumerate}    \item bayesian framework $=$ allows working with large number of basis          functions. Overfitting avoidance $=$ prior keeps weights small    \item open question: how to choose parameters for prior? small $\alpha = $ overfitting, large $\alpha =$ prior dominates posterior: biased output.    \item choosing optimal number of basis functions or finding optimal          $\alpha$ $=$ model selection    \item solution: cross-validation. For regression: common loss function is          squared loss\end{enumerate}[/latex]
